{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Jess-Lau/Real-Life-B-W-Video-Colorization-Project/blob/main/ImageColorizerGANV2.ipynb",
      "authorship_tag": "ABX9TyOkU4xoJrvk8E0k8/Zv7cOT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jess-Lau/Real-Life-B-W-Video-Colorization-Project/blob/main/ColorizerGANV5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ads3J-VraLRG"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Image Colorization GAN\n",
        "# ----------------------\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Configuration\n",
        "# ------------------\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS = 1\n",
        "EPOCHS = 70\n",
        "BATCH_SIZE = 256\n",
        "LAMBDA = 100\n",
        "DATA_DIR = \"/content/drive/MyDrive/ImageNet\"  # Update with your path\n",
        "WORKDIR = \"/content/drive/MyDrive/Colorization\"\n",
        "CHECKPOINT_DIR = os.path.join(WORKDIR, \"checkpoints\")\n",
        "RESULTS_DIR = os.path.join(WORKDIR, \"results\")\n",
        "\n",
        "# Enable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "1jffqiuUahQR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Data Pipeline\n",
        "# ------------------\n",
        "def load_mean(data_dir):\n",
        "    \"\"\"Load mean image from first training batch\"\"\"\n",
        "    with open(os.path.join(data_dir, 'train_data_batch_1'), 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        mean = data['mean'].astype(np.float32) / 255.0\n",
        "        return mean.reshape(3, IMAGE_SIZE, IMAGE_SIZE).transpose(1, 2, 0)\n",
        "\n",
        "def data_generator(data_dir, split='train'):\n",
        "    mean = load_mean(data_dir) if split == 'train' else None\n",
        "    files = [f'train_data_batch_{i}' for i in range(1, 11)] if split == 'train' else ['val_data']\n",
        "\n",
        "    for file in files:\n",
        "        path = os.path.join(data_dir, file)\n",
        "        try:\n",
        "            with open(path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                x = data['data'].astype(np.float32) / 255.0\n",
        "                x = x.reshape(-1, 3, IMAGE_SIZE, IMAGE_SIZE).transpose(0, 2, 3, 1)\n",
        "\n",
        "                if mean is not None:\n",
        "                    x -= mean\n",
        "\n",
        "                for i in range(0, x.shape[0], BATCH_SIZE):\n",
        "                    batch_rgb = x[i:i+BATCH_SIZE]\n",
        "                    batch_lab = np.array([rgb2lab(img) for img in batch_rgb])\n",
        "                    L = batch_lab[..., 0:1].astype(np.float32)\n",
        "                    AB = (batch_lab[..., 1:] / 128.0).astype(np.float32)\n",
        "                    yield L, AB\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load {path}: {str(e)}\")\n",
        "            continue  # Skip problematic files\n",
        "\n",
        "def create_dataset(data_dir, split='train'):\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(data_dir, split),\n",
        "        output_signature=(  # ✅ Proper parentheses\n",
        "            tf.TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None, 64, 64, 2), dtype=tf.float32)\n",
        "        )\n",
        "    ).prefetch(tf.data.AUTOTUNE)  # ✅ .prefetch() called on dataset"
      ],
      "metadata": {
        "id": "oXsq8r77afW4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Model Architectures\n",
        "# ------------------\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    model = Sequential()\n",
        "    model.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                          kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    return model\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    model = Sequential()\n",
        "    model.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                    kernel_initializer=initializer, use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.ReLU())\n",
        "    return model\n",
        "\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
        "\n",
        "    # Encoder\n",
        "    d1 = downsample(64, 4, False)(inputs)    # 32x32\n",
        "    d2 = downsample(128, 4)(d1)              # 16x16\n",
        "    d3 = downsample(256, 4)(d2)              # 8x8\n",
        "    d4 = downsample(512, 4)(d3)              # 4x4\n",
        "\n",
        "    # Decoder\n",
        "    u1 = upsample(512, 4, True)(d4)          # 8x8\n",
        "    u1 = layers.Concatenate()([u1, d3])\n",
        "    u2 = upsample(256, 4)(u1)                # 16x16\n",
        "    u2 = layers.Concatenate()([u2, d2])\n",
        "    u3 = upsample(128, 4)(u2)                # 32x32\n",
        "    u3 = layers.Concatenate()([u3, d1])\n",
        "    u4 = upsample(64, 4)(u3)                 # 64x64\n",
        "\n",
        "    output = layers.Conv2D(2, 3, padding='same', activation='tanh')(u4)\n",
        "    return Model(inputs, output)\n",
        "\n",
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "\n",
        "    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "    return Model(inputs, x)"
      ],
      "metadata": {
        "id": "6YOSJkmZadOh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Setup\n",
        "# ------------------\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=generator_optimizer,\n",
        "    discriminator_optimizer=discriminator_optimizer,\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        "    epoch=tf.Variable(0)\n",
        ")\n",
        "manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_DIR, max_to_keep=3)"
      ],
      "metadata": {
        "id": "ENkU2C05abcZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Utilities\n",
        "# ------------------\n",
        "def generate_images(model, test_input, epoch):\n",
        "    input_L = test_input[0]  # ✅ Extract L channel\n",
        "    target_AB = test_input[1]  # Ground truth AB\n",
        "\n",
        "    # Predict using only L\n",
        "    prediction = model(input_L, training=False)[0].numpy()\n",
        "    L = input_L[0].numpy()[..., 0]  # Use first sample in batch\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Input (grayscale)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(L, cmap='gray')\n",
        "    plt.title(\"Input\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth (colorized)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    true_rgb = lab2rgb(np.dstack((L, target_AB[0].numpy() * 128)))  # ✅ Use target_AB\n",
        "    plt.imshow(true_rgb)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Predicted (colorized)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    pred_rgb = lab2rgb(np.dstack((L, prediction * 128)))\n",
        "    plt.imshow(pred_rgb)\n",
        "    plt.title(\"Predicted\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, f'epoch_{epoch+1}.png'))\n",
        "    plt.close()\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_L, input_AB):\n",
        "    # Cast to mixed precision\n",
        "    input_L = tf.cast(input_L, tf.float16)\n",
        "    input_AB = tf.cast(input_AB, tf.float16)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        generated_AB = generator(input_L, training=True)\n",
        "\n",
        "        # Create concatenated images\n",
        "        real_images = tf.concat([input_L, input_AB], axis=-1)\n",
        "        fake_images = tf.concat([input_L, generated_AB], axis=-1)\n",
        "\n",
        "        # Discriminator outputs\n",
        "        disc_real = discriminator(real_images, training=True)\n",
        "        disc_fake = discriminator(fake_images, training=True)\n",
        "\n",
        "        # Loss calculations\n",
        "        gen_loss = tf.keras.losses.binary_crossentropy(\n",
        "            tf.ones_like(disc_fake), disc_fake) + LAMBDA * tf.reduce_mean(tf.abs(input_AB - generated_AB))\n",
        "        disc_loss = tf.keras.losses.binary_crossentropy(\n",
        "            tf.ones_like(disc_real), disc_real) + tf.keras.losses.binary_crossentropy(\n",
        "            tf.zeros_like(disc_fake), disc_fake)\n",
        "\n",
        "    # Apply gradient clipping\n",
        "    gen_grads = tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gen_grads = [tf.clip_by_norm(g, 1.0) for g in gen_grads]\n",
        "    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "\n",
        "    disc_grads = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    disc_grads = [tf.clip_by_norm(g, 1.0) for g in disc_grads]\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zs6PxIdJaVn0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Loop\n",
        "# ------------------\n",
        "def train():\n",
        "    train_dataset = create_dataset(DATA_DIR, 'train')\n",
        "    val_dataset = create_dataset(DATA_DIR, 'val')\n",
        "\n",
        "    if manager.latest_checkpoint:\n",
        "        checkpoint.restore(manager.latest_checkpoint)\n",
        "        print(f\"Resumed from epoch {checkpoint.epoch.numpy()}\")\n",
        "\n",
        "    for epoch in range(checkpoint.epoch.numpy(), EPOCHS):\n",
        "        start = time.time()\n",
        "        gen_losses, disc_losses = [], []\n",
        "\n",
        "        for batch, (L, AB) in enumerate(train_dataset):\n",
        "            gen_loss, disc_loss = train_step(L, AB)\n",
        "            gen_losses.append(gen_loss)\n",
        "            disc_losses.append(disc_loss)\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                gen_loss_val = gen_loss.numpy().item()\n",
        "                disc_loss_val = disc_loss.numpy().item()\n",
        "                print(f\"Epoch {epoch+1} Batch {batch} | Gen: {gen_loss_val:.2f} Disc: {disc_loss_val:.2f}\")\n",
        "                tf.keras.backend.clear_session()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            manager.save()\n",
        "            test_batch = next(iter(val_dataset))\n",
        "            generate_images(generator, test_batch, epoch)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(f\"Time: {time.time()-start:.2f}s\")\n",
        "        print(f\"Gen Loss: {np.mean(gen_losses):.4f}\")\n",
        "        print(f\"Disc Loss: {np.mean(disc_losses):.4f}\\n\")\n",
        "        checkpoint.epoch.assign_add(1)"
      ],
      "metadata": {
        "id": "BOasui2daTd5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "LvKNoJXlaRZl",
        "outputId": "e733e7de-c602-4067-dc86-b54cbbe3bed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed from epoch 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_util.py:511: RuntimeWarning: overflow encountered in cast\n",
            "  nparray = values.astype(dtype.as_numpy_dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70 Batch 0 | Gen: 9.85 Disc: 1.39\n",
            "Epoch 70 Batch 100 | Gen: 10.69 Disc: 1.14\n",
            "Epoch 70 Batch 200 | Gen: 9.63 Disc: 1.33\n",
            "Epoch 70 Batch 300 | Gen: 9.67 Disc: 1.32\n",
            "Epoch 70 Batch 400 | Gen: 9.84 Disc: 1.12\n",
            "Epoch 70 Batch 500 | Gen: 10.32 Disc: 1.16\n",
            "Epoch 70 Batch 600 | Gen: 9.74 Disc: 1.30\n",
            "Epoch 70 Batch 700 | Gen: 10.37 Disc: 1.17\n",
            "Epoch 70 Batch 800 | Gen: 9.73 Disc: 1.37\n",
            "Epoch 70 Batch 900 | Gen: 9.32 Disc: 1.54\n",
            "Epoch 70 Batch 1000 | Gen: 10.48 Disc: 1.23\n",
            "Epoch 70 Batch 1100 | Gen: 9.77 Disc: 1.17\n",
            "Epoch 70 Batch 1200 | Gen: 9.82 Disc: 1.30\n",
            "Epoch 70 Batch 1300 | Gen: 10.13 Disc: 1.30\n",
            "Epoch 70 Batch 1400 | Gen: 10.53 Disc: 1.03\n",
            "Epoch 70 Batch 1500 | Gen: 9.41 Disc: 1.22\n",
            "Epoch 70 Batch 1600 | Gen: 10.32 Disc: 1.33\n",
            "Epoch 70 Batch 1700 | Gen: 10.17 Disc: 1.23\n",
            "Epoch 70 Batch 1800 | Gen: 10.02 Disc: 1.28\n",
            "Epoch 70 Batch 1900 | Gen: 10.55 Disc: 1.38\n",
            "Epoch 70 Batch 2000 | Gen: 10.23 Disc: 1.25\n",
            "Epoch 70 Batch 2100 | Gen: 10.34 Disc: 1.32\n",
            "Epoch 70 Batch 2200 | Gen: 9.78 Disc: 1.13\n",
            "Epoch 70 Batch 2300 | Gen: 10.27 Disc: 1.44\n",
            "Epoch 70 Batch 2400 | Gen: 10.02 Disc: 1.45\n",
            "Epoch 70 Batch 2500 | Gen: 9.80 Disc: 1.20\n",
            "Epoch 70 Batch 2600 | Gen: 10.09 Disc: 1.09\n",
            "Epoch 70 Batch 2700 | Gen: 10.36 Disc: 1.25\n",
            "Epoch 70 Batch 2800 | Gen: 10.01 Disc: 1.29\n",
            "Epoch 70 Batch 2900 | Gen: 10.05 Disc: 1.26\n",
            "Epoch 70 Batch 3000 | Gen: 10.42 Disc: 1.36\n",
            "Epoch 70 Batch 3100 | Gen: 10.21 Disc: 1.29\n",
            "Epoch 70 Batch 3200 | Gen: 10.25 Disc: 1.22\n",
            "Epoch 70 Batch 3300 | Gen: 9.74 Disc: 1.30\n",
            "Epoch 70 Batch 3400 | Gen: 10.58 Disc: 1.29\n",
            "Epoch 70 Batch 3500 | Gen: 10.18 Disc: 1.27\n",
            "Epoch 70 Batch 3600 | Gen: 10.20 Disc: 1.20\n",
            "Epoch 70 Batch 3700 | Gen: 9.30 Disc: 1.22\n",
            "Epoch 70 Batch 3800 | Gen: 9.79 Disc: 1.29\n",
            "Epoch 70 Batch 3900 | Gen: 10.08 Disc: 1.29\n",
            "Epoch 70 Batch 4000 | Gen: 10.90 Disc: 1.26\n",
            "Epoch 70 Batch 4100 | Gen: 9.95 Disc: 1.21\n",
            "Epoch 70 Batch 4200 | Gen: 10.34 Disc: 1.30\n",
            "Epoch 70 Batch 4300 | Gen: 10.11 Disc: 1.31\n",
            "Epoch 70 Batch 4400 | Gen: 10.31 Disc: 1.31\n",
            "Epoch 70 Batch 4500 | Gen: 10.47 Disc: 1.37\n",
            "Epoch 70 Batch 4600 | Gen: 9.41 Disc: 1.55\n",
            "Epoch 70 Batch 4700 | Gen: 10.59 Disc: 1.13\n",
            "Epoch 70 Batch 4800 | Gen: 9.45 Disc: 1.24\n",
            "Epoch 70 Batch 4900 | Gen: 10.19 Disc: 1.29\n",
            "Epoch 70 Batch 5000 | Gen: 11.00 Disc: 1.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-900d856eca63>:29: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 21 negative Z values that have been clipped to zero\n",
            "  pred_rgb = lab2rgb(np.dstack((L, prediction * 128)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 70/70\n",
            "Time: 1357.40s\n",
            "Gen Loss: 10.1016\n",
            "Disc Loss: 1.2832\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Inference Function\n",
        "# ----------------------\n",
        "def colorize_image(model, image_path, output_path, image_size=64):\n",
        "    \"\"\"\n",
        "    Colorizes a single image using the trained generator.\n",
        "\n",
        "    Args:\n",
        "        model: Trained generator model\n",
        "        image_path: Path to input grayscale/RGB image\n",
        "        output_path: Path to save colorized image\n",
        "        image_size: Size to resize image (must match model input)\n",
        "    \"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not load image at {image_path}\")\n",
        "\n",
        "    # Convert to RGB if needed\n",
        "    if image.ndim == 2:  # Grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    else:  # BGR to RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize and normalize\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Convert to LAB and extract L channel\n",
        "    lab = rgb2lab(image)\n",
        "    L = lab[:, :, 0:1]  # (H, W, 1)\n",
        "\n",
        "    # Add batch dimension and predict\n",
        "    L_batch = np.expand_dims(L, axis=0)  # (1, H, W, 1)\n",
        "    AB_pred = model.predict(L_batch, verbose=0)[0]  # (H, W, 2)\n",
        "\n",
        "    # Denormalize AB channels\n",
        "    AB_pred = (AB_pred * 128.0).astype(np.float32)\n",
        "\n",
        "    # Combine with L and convert to RGB\n",
        "    colorized_lab = np.concatenate([L, AB_pred], axis=-1)\n",
        "    colorized_rgb = lab2rgb(colorized_lab)\n",
        "\n",
        "    # Clip and save\n",
        "    colorized_rgb = np.clip(colorized_rgb, 0, 1)\n",
        "    plt.imsave(output_path, colorized_rgb)\n",
        "    print(f\"Colorized image saved to {output_path}\")"
      ],
      "metadata": {
        "id": "PmQiENqcR8bz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, use like this:\n",
        "colorize_image(\n",
        "    generator,\n",
        "    \"/content/my_grayscale_image.jpg\",  # Input path\n",
        "    \"/content/colorized_result.jpg\"     # Output path\n",
        ")"
      ],
      "metadata": {
        "id": "TfAazZuISFbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Parallel Video Colorization with Temporal Consistency\n",
        "# ----------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "# ----------------------\n",
        "# Video Colorizer with Content Directory Temp Files\n",
        "# ----------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "class VideoColorizer:\n",
        "    def __init__(self, model, temporal_alpha=0.8, blend_factor=0.7):\n",
        "        self.model = model\n",
        "        self.temporal_alpha = temporal_alpha\n",
        "        self.blend_factor = blend_factor\n",
        "        # Optical flow parameters\n",
        "        self.flow_params = {\n",
        "            'pyr_scale': 0.5,\n",
        "            'levels': 3,\n",
        "            'winsize': 15,\n",
        "            'iterations': 3,\n",
        "            'poly_n': 5,\n",
        "            'poly_sigma': 1.2,\n",
        "            'flags': cv2.OPTFLOW_FARNEBACK_GAUSSIAN\n",
        "        }\n",
        "        # Create temp directory in Colab's content directory\n",
        "        self.temp_dir = tempfile.TemporaryDirectory(\n",
        "            dir='/content',\n",
        "            prefix='colorizer_temp_'\n",
        "        )\n",
        "        print(f\"Temporary directory created at: {self.temp_dir.name}\")\n",
        "        os.sync()  # Flush file buffers\n",
        "\n",
        "\n",
        "\n",
        "    def _process_chunk(self, frames, start_idx):\n",
        "        \"\"\"Process a chunk of frames in parallel\"\"\"\n",
        "        try:\n",
        "            print(f\"Processing chunk starting at index {start_idx}\")\n",
        "            os.makedirs(self.temp_dir.name, exist_ok=True)\n",
        "\n",
        "            for idx, frame in enumerate(frames):\n",
        "                frame_idx = start_idx + idx\n",
        "                save_path = os.path.join(\n",
        "                    self.temp_dir.name,\n",
        "                    f\"frame_{frame_idx:06d}.npy\"  # Consistent \"frame_\" prefix\n",
        "                )\n",
        "\n",
        "                # Colorization pipeline\n",
        "                resized_rgb = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (64, 64))\n",
        "                lab = rgb2lab(resized_rgb.astype(np.float32)/255.0)\n",
        "                L = lab[:, :, 0:1]\n",
        "                AB = self.model.predict(np.expand_dims(L, axis=0), verbose=0)[0]\n",
        "\n",
        "                # Save with explicit path\n",
        "                data = {\n",
        "                    'frame': frame,\n",
        "                    'L': L,\n",
        "                    'AB': AB,\n",
        "                    'gray': cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                }\n",
        "                np.save(save_path, data)\n",
        "                print(f\"Saved frame {frame_idx} to {save_path}\")\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chunk {start_idx}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _temporal_smooth(self, prev_data, current_data):\n",
        "        \"\"\"Apply temporal consistency between frames\"\"\"\n",
        "        if prev_data is None:\n",
        "            return current_data['AB']\n",
        "\n",
        "        # Compute optical flow at model resolution\n",
        "        target_size = (64, 64)\n",
        "        prev_gray = cv2.resize(prev_data['gray'], target_size)\n",
        "        current_gray = cv2.resize(current_data['gray'], target_size)\n",
        "\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            prev_gray, current_gray,\n",
        "            None, **self.flow_params\n",
        "        )\n",
        "\n",
        "        # Create normalized coordinate grid\n",
        "        h, w = target_size\n",
        "        x_map, y_map = np.meshgrid(np.arange(w), np.arange(h))\n",
        "        flow_map = np.stack([\n",
        "           (x_map + flow[..., 0]).astype(np.float32),\n",
        "           (y_map + flow[..., 1]).astype(np.float32)\n",
        "        ], axis=-1)\n",
        "\n",
        "        # Ensure coordinates stay within image bounds\n",
        "        flow_map[..., 0] = np.clip(flow_map[..., 0], 0, w-1)\n",
        "        flow_map[..., 1] = np.clip(flow_map[..., 1], 0, h-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Warp previous AB channels\n",
        "        warped_AB = cv2.remap(\n",
        "            prev_data['AB'].astype(np.float32),  # Ensure float32 input\n",
        "            flow_map,\n",
        "            None,\n",
        "            cv2.INTER_LINEAR,\n",
        "            borderMode=cv2.BORDER_REFLECT\n",
        "        )\n",
        "\n",
        "        # Handle invalid regions (black borders from warping)\n",
        "        mask = (warped_AB == 0).all(axis=-1, keepdims=True)\n",
        "        blended_AB = np.where(mask, current_data['AB'],\n",
        "                         self.blend_factor * current_data['AB'] +\n",
        "                         (1 - self.blend_factor) * warped_AB)\n",
        "\n",
        "        smoothed_AB = self.temporal_alpha * blended_AB + \\\n",
        "                     (1 - self.temporal_alpha) * warped_AB\n",
        "        return smoothed_AB\n",
        "\n",
        "    def colorize_video(self, input_path, output_path,\n",
        "                     batch_size=16, workers=8,\n",
        "                     frame_limit=None):\n",
        "        # Initialization checks\n",
        "        print(f\"Initializing video processing\")\n",
        "        print(f\"Input: {input_path}\")\n",
        "        print(f\"Output: {output_path}\")\n",
        "\n",
        "        \"\"\"Main processing pipeline\"\"\"\n",
        "        # Phase 1: Parallel colorization\n",
        "        print(f\"Temporary directory: {self.temp_dir.name}\")\n",
        "        if not os.path.exists(self.temp_dir.name):\n",
        "            raise FileNotFoundError(\"Temp directory not created!\")\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            print(\"❌ Failed to open input video\")\n",
        "            return\n",
        "\n",
        "        executor = ThreadPoolExecutor(max_workers=workers)\n",
        "        futures = []\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        print(f\"Total frames: {frame_count}\")\n",
        "        print(f\"FPS: {cap.get(cv2.CAP_PROP_FPS)}\")\n",
        "        print(f\"Resolution: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}x{cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}\")\n",
        "        chunk = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or (frame_limit and frame_count >= frame_limit):\n",
        "                break\n",
        "\n",
        "            chunk.append(frame)\n",
        "            frame_count += 1\n",
        "\n",
        "            if len(chunk) == batch_size:\n",
        "                future = executor.submit(self._process_chunk, chunk.copy(), frame_count - batch_size)\n",
        "                futures.append(future)\n",
        "                chunk = []\n",
        "\n",
        "        # Process remaining frames\n",
        "        if len(chunk) > 0:\n",
        "            future = executor.submit(self._process_chunk, chunk, frame_count - len(chunk))\n",
        "            futures.append(future)\n",
        "\n",
        "        # Wait for all chunks to complete\n",
        "        _ = [f.result() for f in futures]\n",
        "        cap.release()\n",
        "\n",
        "        # Phase 2: Temporal smoothing and output\n",
        "        writer = cv2.VideoWriter(output_path,\n",
        "                               cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                               cap.get(cv2.CAP_PROP_FPS),\n",
        "                               (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                               int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "        prev_data = None\n",
        "        saved_files = sorted([\n",
        "            f for f in os.listdir(self.temp_dir.name)\n",
        "            if f.startswith(\"frame_\") and f.endswith(\".npy\")\n",
        "        ], key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "\n",
        "        print(f\"Found {len(saved_files)} processed frames\")\n",
        "        print(\"Sample files:\", saved_files[:3])\n",
        "\n",
        "        for i, filename in enumerate(saved_files):\n",
        "            file_path = os.path.join(self.temp_dir.name, filename)\n",
        "            data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "        for i in range(frame_count):\n",
        "            # Use zero-padded filenames\n",
        "            file_path = os.path.join(self.temp_dir.name, f\"{i:06d}.npy\")\n",
        "            if not os.path.exists(file_path):\n",
        "                raise FileNotFoundError(f\"Missing frame {i}: {file_path}\")\n",
        "\n",
        "            data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "            # Apply temporal smoothing\n",
        "            smoothed_AB = self._temporal_smooth(prev_data, data)\n",
        "\n",
        "            # Generate final frame\n",
        "            lab = np.concatenate([rgb2lab(cv2.resize(cv2.cvtColor(data['frame'],cv2.COLOR_BGR2RGB),(64, 64)))[..., 0:1],smoothed_AB],axis=-1)\n",
        "            # Resize back to original dimensions\n",
        "            rgb = (lab2rgb(lab) * 255).astype(np.uint8)\n",
        "            final_frame = cv2.resize(rgb, (data['frame'].shape[1], data['frame'].shape[0]))\n",
        "            print(f\"First frame shape before write: {final_frame.shape}\")\n",
        "            writer.write(final_frame)\n",
        "            print(\"Successfully wrote first frame\")\n",
        "            prev_data = {'AB': smoothed_AB, 'gray': data['gray']}\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Processed {i+1}/{frame_count} frames\")\n",
        "        print(f\"Generated {len(os.listdir(self.temp_dir.name))} intermediate files\")\n",
        "        writer.release()\n",
        "        self.temp_dir.cleanup()\n"
      ],
      "metadata": {
        "id": "6FYBJCluSoP8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colorizer = VideoColorizer(generator, temporal_alpha=0.85)\n",
        "\n",
        "\n",
        "colorizer.colorize_video(\n",
        "    \"/content/Video/input.mp4\",\n",
        "    \"/content/Video/output.mp4\",\n",
        "    batch_size=32,\n",
        "    workers=8,\n",
        "    frame_limit=300\n",
        ")"
      ],
      "metadata": {
        "id": "LiFkW4nLYBbb",
        "outputId": "5a4a33cc-2144-45ca-cd18-12477de74db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporary directory created at: /content/colorizer_temp_8yti6knv\n",
            "Initializing video processing\n",
            "Input: /content/Video/input.mp4\n",
            "Output: /content/Video/output.mp4\n",
            "Temporary directory: /content/colorizer_temp_8yti6knv\n",
            "Total frames: 324\n",
            "FPS: 29.97002997002997\n",
            "Resolution: 1280.0x720.0\n",
            "Found 0 processed frames\n",
            "Sample files: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Missing frame 0: /content/colorizer_temp_8yti6knv/000000.npy",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-5e962d1868e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m colorizer.colorize_video(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"/content/Video/input.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"/content/Video/output.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-bc7ed9df49d0>\u001b[0m in \u001b[0;36mcolorize_video\u001b[0;34m(self, input_path, output_path, batch_size, workers, frame_limit)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{i:06d}.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing frame {i}: {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Missing frame 0: /content/colorizer_temp_8yti6knv/000000.npy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tempfile.TemporaryDirectory(\n",
        "            dir='/content',\n",
        "            prefix='colorizer_temp_'\n",
        "        )"
      ],
      "metadata": {
        "id": "JXS8Ko74u4qZ",
        "outputId": "1166857b-ee21-4a77-da27-47f51ac47f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TemporaryDirectory '/content/colorizer_temp_9sxw912w'>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find optimal batch size for your GPU\n",
        "!nvidia-smi --loop=1  # Monitor GPU memory usage"
      ],
      "metadata": {
        "id": "QXs5HGFfYII3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}