{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "https://github.com/Jess-Lau/Real-Life-B-W-Video-Colorization-Project/blob/main/ImageColorizerGANV2.ipynb",
      "authorship_tag": "ABX9TyPEVgjcIaztLlHRdeMOiY5T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jess-Lau/Real-Life-B-W-Video-Colorization-Project/blob/main/ColorizerGANV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow tensorflow_addons keras\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99tOhSklXU8f",
        "outputId": "cd071d37-dca5-498b-deb0-1f08f7f56ebc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "Found existing installation: tensorflow-addons 0.23.0\n",
            "Uninstalling tensorflow-addons-0.23.0:\n",
            "  Successfully uninstalled tensorflow-addons-0.23.0\n",
            "Found existing installation: keras 2.15.0\n",
            "Uninstalling keras-2.15.0:\n",
            "  Successfully uninstalled keras-2.15.0\n",
            "Collecting tensorflow==2.15.0\n",
            "  Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.13.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Installing collected packages: keras, tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 tensorflow-2.15.0\n",
            "Collecting tensorflow-addons\n",
            "  Using cached tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Using cached tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ads3J-VraLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec17c16-a73f-4554-f67e-c3b4d98ef3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# Colorization GAN\n",
        "# ----------------------\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers, Model, Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Configuration\n",
        "# ------------------\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS = 1\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 256\n",
        "LAMBDA = 100\n",
        "DATA_DIR = \"/content/drive/MyDrive/ImageNet\"\n",
        "WORKDIR = \"/content/drive/MyDrive/Colorization\"\n",
        "CHECKPOINT_DIR = os.path.join(WORKDIR, \"checkpoints\")\n",
        "RESULTS_DIR = os.path.join(WORKDIR, \"results\")\n",
        "\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "tf.debugging.enable_check_numerics()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "1jffqiuUahQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc4be44-7b95-47f2-ca62-850c5c6f31a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Data Pipeline\n",
        "# ------------------\n",
        "def load_mean(data_dir):\n",
        "    \"\"\"Load mean image from first training batch\"\"\"\n",
        "    with open(os.path.join(data_dir, 'train_data_batch_1'), 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        mean = data['mean'].astype(np.float32) / 255.0\n",
        "        return mean.reshape(3, IMAGE_SIZE, IMAGE_SIZE).transpose(1, 2, 0)\n",
        "\n",
        "def data_generator(data_dir, split='train'):\n",
        "    mean = load_mean(data_dir) if split == 'train' else None\n",
        "    files = [f'train_data_batch_{i}' for i in range(1, 11)] if split == 'train' else ['val_data']\n",
        "\n",
        "    for file in files:\n",
        "        path = os.path.join(data_dir, file)\n",
        "        try:\n",
        "            with open(path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                x = data['data'].astype(np.float32) / 255.0\n",
        "                x = x.reshape(-1, 3, IMAGE_SIZE, IMAGE_SIZE).transpose(0, 2, 3, 1)\n",
        "\n",
        "                if mean is not None:\n",
        "                    x -= mean\n",
        "\n",
        "                for i in range(0, x.shape[0], BATCH_SIZE):\n",
        "                    batch_rgb = x[i:i+BATCH_SIZE]\n",
        "                    batch_lab = np.array([rgb2lab(img) for img in batch_rgb])\n",
        "                    L = batch_lab[..., 0:1].astype(np.float32)\n",
        "                    AB = batch_lab[..., 1:] / 128.0\n",
        "                    AB = np.clip(AB, -1.0, 1.0).astype(np.float32)  # <-- Critical fix\n",
        "                    yield L, AB\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load {path}: {str(e)}\")\n",
        "            continue  # Skip problematic files\n",
        "\n",
        "def create_dataset(data_dir, split='train'):\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(data_dir, split),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None, 64, 64, 2), dtype=tf.float32)\n",
        "        )\n",
        "    ).map(lambda L, AB: (tf.cast(L, tf.float16), tf.cast(AB, tf.float16)),\n",
        "              num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).cache()  # Cache after first epoch"
      ],
      "metadata": {
        "id": "oXsq8r77afW4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Model Architectures\n",
        "# ------------------\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    model = Sequential()\n",
        "    model.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                          kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    return model\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    model = Sequential()\n",
        "    model.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                    kernel_initializer=initializer, use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.ReLU())\n",
        "    return model\n",
        "\n",
        "def residual_block(x, output_filters, kernel_size=3):\n",
        "    \"\"\"Residual block with channel projection for shortcut\"\"\"\n",
        "    input_filters = x.shape[-1]\n",
        "    shortcut = x\n",
        "\n",
        "    # Main path\n",
        "    x = layers.Conv2D(output_filters, kernel_size, padding='same')(x)\n",
        "    x = tfa.layers.InstanceNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(output_filters, kernel_size, padding='same')(x)\n",
        "    x = tfa.layers.InstanceNormalization()(x)\n",
        "\n",
        "    # Shortcut path (1x1 conv if channels change)\n",
        "    if input_filters != output_filters:\n",
        "        shortcut = layers.Conv2D(output_filters, 1, padding='same')(shortcut)\n",
        "        shortcut = tfa.layers.InstanceNormalization()(shortcut)\n",
        "\n",
        "    return layers.add([shortcut, x])\n",
        "\n",
        "def attention_block(x, filters):\n",
        "    \"\"\"Self-attention layer with proper dynamic shape handling\"\"\"\n",
        "    # Get dynamic dimensions using tf.shape()\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    h = tf.shape(x)[1]\n",
        "    w = tf.shape(x)[2]\n",
        "\n",
        "    # Query/Key/Value projections\n",
        "    f = layers.Conv2D(filters//8, 1, padding='same')(x)  # query\n",
        "    g = layers.Conv2D(filters//8, 1, padding='same')(x)  # key\n",
        "    v = layers.Conv2D(filters, 1, padding='same')(x)     # value\n",
        "\n",
        "    # Reshape with computed dimensions\n",
        "    f_flat = tf.reshape(f, [batch_size, h*w, filters//8])\n",
        "    g_flat = tf.reshape(g, [batch_size, h*w, filters//8])\n",
        "    v_flat = tf.reshape(v, [batch_size, h*w, filters])\n",
        "\n",
        "    # Attention computation\n",
        "    attention = tf.matmul(g_flat, f_flat, transpose_b=True)\n",
        "    attention = tf.nn.softmax(attention, axis=-1)\n",
        "    context = tf.matmul(attention, v_flat)\n",
        "\n",
        "    # Reshape back to spatial dimensions\n",
        "    context = tf.reshape(context, [batch_size, h, w, filters])\n",
        "    return layers.Conv2D(filters, 1, padding='same')(context) + x\n",
        "\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(64, 64, 1))\n",
        "\n",
        "    # Encoder\n",
        "    d1 = downsample(64, 4)(inputs)          # 64x64 → 32x32\n",
        "    d1 = residual_block(d1, 64)\n",
        "\n",
        "    d2 = downsample(128, 4)(d1)             # 32x32 → 16x16\n",
        "    d2 = residual_block(d2, 128)\n",
        "\n",
        "    d3 = downsample(256, 4)(d2)             # 16x16 → 8x8\n",
        "    d3 = attention_block(d3, 256)\n",
        "    d3 = residual_block(d3, 256)\n",
        "\n",
        "    # Bottleneck\n",
        "    d4 = downsample(512, 4)(d3)             # 8x8 → 4x4\n",
        "    d4 = residual_block(d4, 512)\n",
        "    d4 = residual_block(d4, 512)\n",
        "\n",
        "    # Decoder with proper channel alignment\n",
        "    u1 = upsample(512, 4)(d4)               # 4x4 → 8x8\n",
        "    u1 = layers.Concatenate()([u1, d3])      # 512 + 256 = 768 channels\n",
        "    u1 = residual_block(u1, 512)             # Project to 512\n",
        "\n",
        "    u2 = upsample(256, 4)(u1)               # 8x8 → 16x16\n",
        "    u2 = layers.Concatenate()([u2, d2])      # 256 + 128 = 384 channels\n",
        "    u2 = residual_block(u2, 256)             # Project to 256\n",
        "\n",
        "    u3 = upsample(128, 4)(u2)               # 16x16 → 32x32\n",
        "    u3 = layers.Concatenate()([u3, d1])      # 128 + 64 = 192 channels\n",
        "    u3 = residual_block(u3, 128)             # Project to 128\n",
        "\n",
        "    u4 = upsample(64, 4)(u3)                # 32x32 → 64x64\n",
        "    u4 = attention_block(u4, 64)\n",
        "\n",
        "    output = layers.Conv2D(2, 3, padding='same',\n",
        "                          activation='tanh',\n",
        "                          dtype=tf.float32)(u4)  # ✅ Critical fix\n",
        "    return Model(inputs, output)\n",
        "\n",
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(64, 64, 3), dtype=tf.float16)\n",
        "\n",
        "    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = tfa.layers.SpectralNormalization(\n",
        "        layers.Conv2D(128, 4, strides=2, padding='same'))(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = tfa.layers.SpectralNormalization(\n",
        "        layers.Conv2D(256, 4, strides=2, padding='same'))(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # PatchGAN output (16x16 \"real/fake\" patches)\n",
        "    x = tfa.layers.SpectralNormalization(\n",
        "        layers.Conv2D(1, 4, padding='same'))(x)\n",
        "\n",
        "    return Model(inputs, x)"
      ],
      "metadata": {
        "id": "6YOSJkmZadOh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Setup\n",
        "# ------------------\n",
        "\n",
        "# Rebuild generator with this wrapper\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Wrap optimizers correctly\n",
        "generator_optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n",
        "    tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        ")\n",
        "discriminator_optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n",
        "    tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        ")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=generator_optimizer,\n",
        "    discriminator_optimizer=discriminator_optimizer,\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        "    epoch=tf.Variable(0)\n",
        ")\n",
        "manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_DIR, max_to_keep=3)"
      ],
      "metadata": {
        "id": "ENkU2C05abcZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Utilities\n",
        "# ------------------\n",
        "def generate_images(model, test_input, epoch):\n",
        "    input_L = test_input[0]  # ✅ Extract L channel\n",
        "    target_AB = test_input[1]  # Ground truth AB\n",
        "\n",
        "    # Predict using only L\n",
        "    prediction = model(input_L, training=False)[0].numpy()\n",
        "    L = input_L[0].numpy()[..., 0]  # Use first sample in batch\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Input (grayscale)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(L, cmap='gray')\n",
        "    plt.title(\"Input\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth (colorized)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    true_rgb = lab2rgb(np.dstack((L, target_AB[0].numpy() * 128)))  # ✅ Use target_AB\n",
        "    plt.imshow(true_rgb)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Predicted (colorized)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    pred_rgb = lab2rgb(np.dstack((L, prediction * 128)))\n",
        "    plt.imshow(pred_rgb)\n",
        "    plt.title(\"Predicted\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, f'epoch_{epoch+1}.png'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# --- Add PSNR/SSIM Calculations ---\n",
        "# Convert LAB to RGB for metrics\n",
        "def lab_to_rgb(lab):\n",
        "    \"\"\"Convert LAB tensor to RGB tensor (0-255 range) with mixed precision support\"\"\"\n",
        "    # Ensure LAB tensor is float32 for stable calculations\n",
        "    lab = tf.cast(lab, tf.float32)\n",
        "\n",
        "    # Denormalize LAB\n",
        "    L = lab[..., 0] * 100.0          # L: [0,100]\n",
        "    ab = lab[..., 1:] * 128.0        # ab: [-128, 127]\n",
        "\n",
        "    # Convert LAB to XYZ\n",
        "    y = (L + 16.0) / 116.0\n",
        "    x = ab[..., 0] / 500.0 + y\n",
        "    z = y - ab[..., 1] / 200.0\n",
        "\n",
        "    xyz = tf.stack([x, y, z], axis=-1)\n",
        "    xyz = tf.where(xyz > 0.2068966, xyz**3, (xyz - 16.0/116.0)/7.787)\n",
        "\n",
        "    # D65 reference white (cast to float32)\n",
        "    xyz = xyz * tf.constant([95.047, 100.0, 108.883], dtype=tf.float32)\n",
        "\n",
        "    # XYZ to RGB matrix\n",
        "    rgb = tf.tensordot(xyz, tf.constant([\n",
        "        [3.2406, -1.5372, -0.4986],\n",
        "        [-0.9689, 1.8758, 0.0415],\n",
        "        [0.0557, -0.2040, 1.0570]\n",
        "    ], dtype=tf.float32), axes=1)\n",
        "\n",
        "    # Gamma correction\n",
        "    rgb = tf.where(rgb > 0.0031308,\n",
        "                    1.055 * (rgb ** (1/2.4)) - 0.055,\n",
        "                    12.92 * rgb)\n",
        "\n",
        "    # Final conversion to float16 if needed\n",
        "    return rgb\n",
        "\n",
        "def safe_psnr(real, fake, max_val=255.0, eps=1e-10):\n",
        "    mse = tf.reduce_mean(tf.square(real - fake))\n",
        "    return 20 * tf.math.log(max_val) / tf.math.log(10.0) - 10 * tf.math.log(mse + eps) / tf.math.log(10.0)\n",
        "\n",
        "# Perceptual Loss (VGG-based)\n",
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False\n",
        "loss_net = Model(vgg.input, vgg.get_layer('block3_conv3').output)\n",
        "\n",
        "def perceptual_loss(real, fake):\n",
        "    real_features = loss_net(real)\n",
        "    fake_features = loss_net(fake)\n",
        "    return tf.reduce_mean(tf.abs(real_features - fake_features))\n",
        "\n",
        "# Feature Matching Loss\n",
        "def feature_matching_loss(real_logits, fake_logits):\n",
        "    return tf.reduce_mean(tf.abs(tf.reduce_mean(real_logits, axis=(1,2)) -\n",
        "                         tf.reduce_mean(fake_logits, axis=(1,2))))\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_L, input_AB):\n",
        "    # Cast to mixed precision\n",
        "    input_L = tf.cast(input_L, tf.float16)\n",
        "    input_AB = tf.cast(input_AB, tf.float16)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        generated_AB = generator(input_L, training=True)\n",
        "        generated_AB = tf.cast(generated_AB, tf.float16)\n",
        "\n",
        "        # Create concatenated images\n",
        "        real_images = tf.concat([input_L, input_AB], axis=-1)\n",
        "        fake_images = tf.concat([input_L, generated_AB], axis=-1)\n",
        "        # Ground truth RGB (from original LAB)\n",
        "        lab_real = tf.concat([input_L, input_AB], axis=-1)\n",
        "        lab_real = tf.cast(lab_real, tf.float32)\n",
        "        rgb_real = lab_to_rgb(lab_real)\n",
        "\n",
        "        # Generated RGB\n",
        "        lab_fake = tf.concat([input_L, generated_AB], axis=-1)\n",
        "        lab_fake = tf.cast(lab_fake, tf.float32)\n",
        "        rgb_fake = lab_to_rgb(lab_fake)\n",
        "\n",
        "        # Discriminator outputs\n",
        "        disc_real = discriminator(real_images, training=True)\n",
        "        disc_fake = discriminator(fake_images, training=True)\n",
        "\n",
        "        # Loss calculations\n",
        "        perc_loss = perceptual_loss(rgb_real, rgb_fake)\n",
        "        fm_loss = feature_matching_loss(disc_real, disc_fake)\n",
        "        gen_loss = tf.keras.losses.binary_crossentropy(\n",
        "            tf.ones_like(disc_fake), disc_fake) + LAMBDA * tf.reduce_mean(tf.abs(input_AB - generated_AB))\n",
        "\n",
        "        scaled_gen_loss = generator_optimizer.get_scaled_loss(gen_loss)\n",
        "\n",
        "        gen_total_loss = scaled_gen_loss + 0.1*perc_loss + 0.1*fm_loss\n",
        "\n",
        "        disc_loss = tf.keras.losses.binary_crossentropy(\n",
        "            tf.ones_like(disc_real), disc_real) + tf.keras.losses.binary_crossentropy(\n",
        "            tf.zeros_like(disc_fake), disc_fake)\n",
        "        scaled_disc_loss = discriminator_optimizer.get_scaled_loss(disc_loss)  # Critical fix\n",
        "\n",
        "\n",
        "    # Calculate metrics\n",
        "    psnr = safe_psnr(rgb_real, rgb_fake, max_val=255.0)\n",
        "    ssim = tf.image.ssim(rgb_real, rgb_fake, max_val=255.0)\n",
        "\n",
        "    scaled_gen_grads = tape.gradient(gen_total_loss, generator.trainable_variables)\n",
        "    gen_grads = generator_optimizer.get_unscaled_gradients(scaled_gen_grads)  # Critical fix\n",
        "    gen_grads = [tf.clip_by_norm(g, 1.0) for g in gen_grads]\n",
        "    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "\n",
        "    # Discriminator gradients\n",
        "    scaled_disc_grads = tape.gradient(scaled_disc_loss, discriminator.trainable_variables)\n",
        "    disc_grads = discriminator_optimizer.get_unscaled_gradients(scaled_disc_grads)  # Critical fix\n",
        "    disc_grads = [tf.clip_by_norm(g, 1.0) for g in disc_grads]\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "    return tf.reduce_mean(gen_total_loss), tf.reduce_mean(scaled_disc_loss), psnr, ssim\n",
        "\n"
      ],
      "metadata": {
        "id": "zs6PxIdJaVn0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def calculate_frame_psnr(original, colorized):\n",
        "    \"\"\"\n",
        "    Calculate PSNR for a single frame pair.\n",
        "    Args:\n",
        "        original: Ground truth frame (BGR)\n",
        "        colorized: Colorized frame (BGR)\n",
        "    Returns:\n",
        "        PSNR value in dB\n",
        "    \"\"\"\n",
        "    # Convert to YCrCb for luminance comparison (optional)\n",
        "    original_yuv = cv2.cvtColor(original, cv2.COLOR_BGR2YCrCb)\n",
        "    colorized_yuv = cv2.cvtColor(colorized, cv2.COLOR_BGR2YCrCb)\n",
        "\n",
        "    # Calculate PSNR for each channel\n",
        "    psnr_y = peak_signal_noise_ratio(original_yuv[...,0], colorized_yuv[...,0], data_range=255)\n",
        "    psnr_cr = peak_signal_noise_ratio(original_yuv[...,1], colorized_yuv[...,1], data_range=255)\n",
        "    psnr_cb = peak_signal_noise_ratio(original_yuv[...,2], colorized_yuv[...,2], data_range=255)\n",
        "\n",
        "    return np.mean([psnr_y, psnr_cr, psnr_cb])\n",
        "\n",
        "def calculate_video_psnr(original_video_path, colorized_video_path):\n",
        "    \"\"\"\n",
        "    Calculate average PSNR between two videos.\n",
        "    Returns:\n",
        "        Mean PSNR (dB), Frame-wise PSNR array\n",
        "    \"\"\"\n",
        "    cap_orig = cv2.VideoCapture(original_video_path)\n",
        "    cap_color = cv2.VideoCapture(colorized_video_path)\n",
        "\n",
        "    psnr_values = []\n",
        "\n",
        "    while True:\n",
        "        ret_orig, frame_orig = cap_orig.read()\n",
        "        ret_color, frame_color = cap_color.read()\n",
        "\n",
        "        if not ret_orig or not ret_color:\n",
        "            break\n",
        "\n",
        "        # Resize if necessary (match dimensions)\n",
        "        if frame_orig.shape != frame_color.shape:\n",
        "            frame_color = cv2.resize(frame_color, (frame_orig.shape[1], frame_orig.shape[0]))\n",
        "\n",
        "        psnr = calculate_frame_psnr(frame_orig, frame_color)\n",
        "        psnr_values.append(psnr)\n",
        "\n",
        "    cap_orig.release()\n",
        "    cap_color.release()\n",
        "\n",
        "    return np.mean(psnr_values), psnr_values"
      ],
      "metadata": {
        "id": "8xpOqmc-3Gko"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Loop\n",
        "# ------------------\n",
        "def train():\n",
        "    train_dataset = create_dataset(DATA_DIR, 'train')\n",
        "    val_dataset = create_dataset(DATA_DIR, 'val')\n",
        "\n",
        "    if manager.latest_checkpoint:\n",
        "        checkpoint.restore(manager.latest_checkpoint)\n",
        "        print(f\"Resumed from epoch {checkpoint.epoch.numpy()}\")\n",
        "\n",
        "    # Initialize metrics\n",
        "    psnr_metric = tf.keras.metrics.Mean(name='psnr')\n",
        "    ssim_metric = tf.keras.metrics.Mean(name='ssim')\n",
        "\n",
        "    for epoch in range(checkpoint.epoch.numpy(), EPOCHS):\n",
        "        start = time.time()\n",
        "        gen_losses, disc_losses = [], []\n",
        "        # Reset metrics each epoch (CORRECTED METHOD NAME)\n",
        "        psnr_metric.reset_state()\n",
        "        ssim_metric.reset_state()\n",
        "\n",
        "        # Training phase\n",
        "        for batch, (L, AB) in enumerate(train_dataset):\n",
        "            gen_loss, disc_loss, psnr, ssim = train_step(L, AB)\n",
        "            gen_losses.append(gen_loss)\n",
        "            disc_losses.append(disc_loss)\n",
        "\n",
        "            if tf.math.is_nan(gen_loss):\n",
        "                print(f\"NaN detected in generator loss at batch {batch}\")\n",
        "                break\n",
        "\n",
        "            # Update metrics\n",
        "            psnr_metric.update_state(psnr)\n",
        "            ssim_metric.update_state(ssim)\n",
        "\n",
        "\n",
        "            # Existing logging\n",
        "            if batch % 100 == 0:\n",
        "                gen_loss_val = gen_loss.numpy().item()\n",
        "                disc_loss_val = disc_loss.numpy().item()\n",
        "                print(f\"Batch {batch} | PSNR: {psnr_metric.result():.2f} | SSIM: {ssim_metric.result():.3f}\")\n",
        "                print(f\"Gen: {gen_loss_val:.2f} Disc: {disc_loss_val:.2f}\")\n",
        "\n",
        "\n",
        "        # In your training loop after the epoch's training phase:\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            manager.save()\n",
        "            test_batch = next(iter(val_dataset))\n",
        "            generate_images(generator, test_batch, epoch)\n",
        "\n",
        "        # Validation phase\n",
        "        val_psnr = []\n",
        "        val_ssim = []\n",
        "        for val_L, val_AB in val_dataset.take(10):\n",
        "           val_gen_AB = generator(val_L, training=False)\n",
        "\n",
        "        # Convert to float32 before concatenation\n",
        "        val_real_lab = tf.concat([\n",
        "            tf.cast(val_L, tf.float32),\n",
        "            tf.cast(val_AB, tf.float32)\n",
        "        ], axis=-1)\n",
        "        val_real_rgb = lab_to_rgb(val_real_lab)\n",
        "\n",
        "        val_fake_lab = tf.concat([\n",
        "            tf.cast(val_L, tf.float32),\n",
        "            tf.cast(val_gen_AB, tf.float32)\n",
        "        ], axis=-1)\n",
        "        val_fake_rgb = lab_to_rgb(val_fake_lab)\n",
        "\n",
        "        # Calculate metrics\n",
        "        batch_psnr = tf.reduce_mean(tf.image.psnr(val_real_rgb, val_fake_rgb, max_val=255))\n",
        "        batch_ssim = tf.reduce_mean(tf.image.ssim(val_real_rgb, val_fake_rgb, max_val=255))\n",
        "\n",
        "        val_psnr.append(batch_psnr.numpy())\n",
        "        val_ssim.append(batch_ssim.numpy())\n",
        "\n",
        "        # Epoch summary\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        print(f\"Time: {time.time()-start:.2f}s\")\n",
        "        print(f\"Gen Loss: {np.mean(gen_losses):.4f}\")\n",
        "        print(f\"Disc Loss: {np.mean(disc_losses):.4f}\\n\")\n",
        "        print(f\"Train PSNR: {psnr_metric.result():.2f} dB\")\n",
        "        print(f\"Train SSIM: {ssim_metric.result():.4f}\")\n",
        "        print(f\"Val PSNR: {np.mean(val_psnr):.2f} dB\")\n",
        "        print(f\"Val SSIM: {np.mean(val_ssim):.4f}\")\n",
        "\n",
        "        checkpoint.epoch.assign_add(1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BOasui2daTd5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "LvKNoJXlaRZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33e40d1-eb0a-48cb-8ea4-f82f8b220a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_util.py:504: RuntimeWarning: overflow encountered in cast\n",
            "  nparray = values.astype(dtype.as_numpy_dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Inference Function\n",
        "# ----------------------\n",
        "def colorize_image(model, image_path, output_path, image_size=64):\n",
        "    \"\"\"\n",
        "    Colorizes a single image using the trained generator.\n",
        "\n",
        "    Args:\n",
        "        model: Trained generator model\n",
        "        image_path: Path to input grayscale/RGB image\n",
        "        output_path: Path to save colorized image\n",
        "        image_size: Size to resize image (must match model input)\n",
        "    \"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not load image at {image_path}\")\n",
        "\n",
        "    # Convert to RGB if needed\n",
        "    if image.ndim == 2:  # Grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    else:  # BGR to RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize and normalize\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Convert to LAB and extract L channel\n",
        "    lab = rgb2lab(image)\n",
        "    L = lab[:, :, 0:1]  # (H, W, 1)\n",
        "\n",
        "    # Add batch dimension and predict\n",
        "    L_batch = np.expand_dims(L, axis=0)  # (1, H, W, 1)\n",
        "    AB_pred = model.predict(L_batch, verbose=0)[0]  # (H, W, 2)\n",
        "\n",
        "    # Denormalize AB channels\n",
        "    AB_pred = (AB_pred * 128.0).astype(np.float32)\n",
        "\n",
        "    # Combine with L and convert to RGB\n",
        "    colorized_lab = np.concatenate([L, AB_pred], axis=-1)\n",
        "    colorized_rgb = lab2rgb(colorized_lab)\n",
        "\n",
        "    # Clip and save\n",
        "    colorized_rgb = np.clip(colorized_rgb, 0, 1)\n",
        "    plt.imsave(output_path, colorized_rgb)\n",
        "    print(f\"Colorized image saved to {output_path}\")"
      ],
      "metadata": {
        "id": "PmQiENqcR8bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, use like this:\n",
        "colorize_image(\n",
        "    generator,\n",
        "    \"/content/my_grayscale_image.jpg\",  # Input path\n",
        "    \"/content/colorized_result.jpg\"     # Output path\n",
        ")"
      ],
      "metadata": {
        "id": "TfAazZuISFbG",
        "outputId": "985168b3-21f5-4aeb-d717-29058e257c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'colorize_image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e730eacfa005>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After training, use like this:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m colorize_image(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"/content/my_grayscale_image.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Input path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"/content/colorized_result.jpg\"\u001b[0m     \u001b[0;31m# Output path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'colorize_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Parallel Video Colorization with Temporal Consistency\n",
        "# ----------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "# ----------------------\n",
        "# Video Colorizer with Content Directory Temp Files\n",
        "# ----------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "class VideoColorizer:\n",
        "    def __init__(self, model, temporal_alpha=0.8, blend_factor=0.7):\n",
        "        self.model = model\n",
        "        self.temporal_alpha = temporal_alpha\n",
        "        self.blend_factor = blend_factor\n",
        "        # Optical flow parameters\n",
        "        self.flow_params = {\n",
        "            'pyr_scale': 0.5,\n",
        "            'levels': 3,\n",
        "            'winsize': 15,\n",
        "            'iterations': 3,\n",
        "            'poly_n': 5,\n",
        "            'poly_sigma': 1.2,\n",
        "            'flags': cv2.OPTFLOW_FARNEBACK_GAUSSIAN\n",
        "        }\n",
        "        # Create temp directory in Colab's content directory\n",
        "        self.temp_dir = tempfile.TemporaryDirectory(\n",
        "            dir='/content',\n",
        "            prefix='colorizer_temp_'\n",
        "        )\n",
        "        os.makedirs(self.temp_dir.name, exist_ok=True)\n",
        "        os.chmod(self.temp_dir.name, 0o777)  # Ensure write permissions\n",
        "        print(f\"Created temp directory at: {self.temp_dir.name}\")\n",
        "\n",
        "    def _process_chunk(self, frames, start_idx):\n",
        "        \"\"\"Process a chunk of frames in parallel\"\"\"\n",
        "        print(f\"Processing chunk starting at {start_idx} with {len(frames)} frames\")\n",
        "\n",
        "        for local_idx, frame in enumerate(frames):\n",
        "            global_idx = start_idx + local_idx\n",
        "            save_path = os.path.join(\n",
        "                self.temp_dir.name,\n",
        "                f\"frame_{global_idx:06d}.npy\"  # Consistent naming\n",
        "            )\n",
        "\n",
        "            # Debug: Verify frame content\n",
        "            if frame is None or frame.size == 0:\n",
        "                raise ValueError(f\"Invalid frame at index {global_idx}\")\n",
        "\n",
        "            # Colorization pipeline\n",
        "            resized_rgb = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (64, 64))\n",
        "            lab = rgb2lab(resized_rgb.astype(np.float32)/255.0)\n",
        "            L = lab[:, :, 0:1]\n",
        "            AB = self.model.predict(np.expand_dims(L, axis=0), verbose=0)[0]\n",
        "\n",
        "            # Save data with verification\n",
        "            data = {\n",
        "                'frame': frame,\n",
        "                'L': L,\n",
        "                'AB': AB,\n",
        "                'gray': cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            }\n",
        "            np.save(save_path, data)\n",
        "            print(f\"Saved frame {global_idx} to {save_path}\")\n",
        "\n",
        "        # Immediate verification of saved files\n",
        "        saved_files = [f for f in os.listdir(self.temp_dir.name)\n",
        "                      if f.startswith(f\"frame_{start_idx:06d}\")]\n",
        "        print(f\"Saved {len(saved_files)} files in this chunk\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _temporal_smooth(self, prev_data, current_data):\n",
        "        \"\"\"Apply temporal consistency between frames\"\"\"\n",
        "        if prev_data is None:\n",
        "            return current_data['AB']\n",
        "\n",
        "        # Compute optical flow at model resolution\n",
        "        target_size = (64, 64)\n",
        "        prev_gray = cv2.resize(prev_data['gray'], target_size)\n",
        "        current_gray = cv2.resize(current_data['gray'], target_size)\n",
        "\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            prev_gray, current_gray,\n",
        "            None, **self.flow_params\n",
        "        )\n",
        "\n",
        "        # Create normalized coordinate grid\n",
        "        h, w = target_size\n",
        "        x_map, y_map = np.meshgrid(np.arange(w), np.arange(h))\n",
        "        flow_map = np.stack([\n",
        "           (x_map + flow[..., 0]).astype(np.float32),\n",
        "           (y_map + flow[..., 1]).astype(np.float32)\n",
        "        ], axis=-1)\n",
        "\n",
        "        # Ensure coordinates stay within image bounds\n",
        "        flow_map[..., 0] = np.clip(flow_map[..., 0], 0, w-1)\n",
        "        flow_map[..., 1] = np.clip(flow_map[..., 1], 0, h-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Warp previous AB channels\n",
        "        warped_AB = cv2.remap(\n",
        "            prev_data['AB'].astype(np.float32),  # Ensure float32 input\n",
        "            flow_map,\n",
        "            None,\n",
        "            cv2.INTER_LINEAR,\n",
        "            borderMode=cv2.BORDER_REFLECT\n",
        "        )\n",
        "\n",
        "        # Handle invalid regions (black borders from warping)\n",
        "        mask = (warped_AB == 0).all(axis=-1, keepdims=True)\n",
        "        blended_AB = np.where(mask, current_data['AB'],\n",
        "                         self.blend_factor * current_data['AB'] +\n",
        "                         (1 - self.blend_factor) * warped_AB)\n",
        "\n",
        "        smoothed_AB = self.temporal_alpha * blended_AB + \\\n",
        "                     (1 - self.temporal_alpha) * warped_AB\n",
        "        return smoothed_AB\n",
        "\n",
        "\n",
        "    def colorize_video(self, input_path, output_path, batch_size=16, workers=8):\n",
        "        # Open video once for all processing\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Couldn't open video {input_path}\")\n",
        "\n",
        "        # Get video properties from the first frame\n",
        "        ret, first_frame = cap.read()\n",
        "        if not ret:\n",
        "            cap.release()\n",
        "            raise ValueError(\"Couldn't read first frame\")\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        print(f\"Width: {frame_width} | Height: {frame_height} | FPS: {fps}\")\n",
        "        # Rewind to beginning\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "        # Initialize video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height), isColor=True)\n",
        "\n",
        "        # Process frames in a single pass\n",
        "        executor = ThreadPoolExecutor(max_workers=workers)\n",
        "        futures = []\n",
        "        chunk = []\n",
        "        frame_count = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            chunk.append(frame)\n",
        "            frame_count += 1\n",
        "            if len(chunk) == batch_size:\n",
        "                future = executor.submit(self._process_chunk, chunk.copy(), frame_count - len(chunk))\n",
        "                futures.append(future)\n",
        "                chunk = []\n",
        "\n",
        "        # Process remaining frames\n",
        "        if chunk:\n",
        "            future = executor.submit(self._process_chunk, chunk, frame_count - len(chunk))\n",
        "            futures.append(future)\n",
        "\n",
        "        # Wait for all processing to finish\n",
        "        for f in futures:\n",
        "            f.result()\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "\n",
        "        # 4. Reconstruct video with temporal smoothing\n",
        "        saved_files = sorted(\n",
        "            [f for f in os.listdir(self.temp_dir.name)\n",
        "             if f.startswith('frame_') and f.endswith('.npy')],\n",
        "            key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "\n",
        "        prev_data = None\n",
        "        for i, filename in enumerate(saved_files):\n",
        "            file_path = os.path.join(self.temp_dir.name, filename)\n",
        "            data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "            # Apply temporal smoothing\n",
        "            smoothed_AB = self._temporal_smooth(prev_data, data)\n",
        "\n",
        "            # Reconstruct frame\n",
        "            rgb_resized = cv2.resize(\n",
        "                cv2.cvtColor(data['frame'], cv2.COLOR_BGR2RGB),\n",
        "                (64, 64))\n",
        "            lab = rgb2lab(rgb_resized.astype(np.float32)/255.0)\n",
        "            final_lab = np.concatenate([lab[..., 0:1], smoothed_AB], axis=-1)\n",
        "\n",
        "            # Convert to output dimensions\n",
        "            colorized_rgb = (lab2rgb(final_lab) * 255).astype(np.uint8)\n",
        "            final_frame = cv2.resize(\n",
        "            colorized_rgb,\n",
        "            (frame_width, frame_height))\n",
        "            final_bgr = cv2.cvtColor(final_frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            writer.write(final_bgr)\n",
        "\n",
        "            prev_data = {'AB': smoothed_AB, 'gray': data['gray']}\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Processed {i+1}/{len(saved_files)} frames\")\n",
        "\n",
        "        # 5. Final cleanup\n",
        "        writer.release()\n",
        "        self.temp_dir.cleanup()\n",
        "\n",
        "        # Verify output\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"\\n✅ Success! Colorized video saved to: {output_path}\")\n",
        "            print(f\"Resolution: {frame_width}x{frame_height} | Frames: {len(saved_files)}\")\n",
        "        else:\n",
        "            print(\"\\n❌ Video creation failed - check codec compatibility\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6FYBJCluSoP8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colorizer = VideoColorizer(generator, temporal_alpha=0.85)\n",
        "\n",
        "\n",
        "colorizer.colorize_video(\n",
        "    \"/content/drive/MyDrive/Colorization/Video/Input.mp4\",\n",
        "    \"/content/drive/MyDrive/Colorization/Video/Output.mp4\",\n",
        "    batch_size=32,\n",
        "    workers=8,\n",
        ")"
      ],
      "metadata": {
        "id": "LiFkW4nLYBbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffprobe -v error -show_entries format=duration \\\n",
        "         -of default=noprint_wrappers=1:nokey=1 \\\n",
        "         \"/content/drive/MyDrive/Colorization/Video/Input2.mp4\""
      ],
      "metadata": {
        "id": "xgdQCtYR15a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffprobe -v error -show_entries format=duration \\\n",
        "         -of default=noprint_wrappers=1:nokey=1 \\\n",
        "         \"/content/drive/MyDrive/Colorization/Video/Output.mp4\""
      ],
      "metadata": {
        "id": "OvhPd2Bn2uPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}