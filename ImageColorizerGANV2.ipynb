{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Jess-Lau/Real-Life-B-W-Video-Colorization-Project/blob/main/ImageColorizerGANV2.ipynb",
      "authorship_tag": "ABX9TyMo+S2ljGbtenoIi1ypPc+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jess-Lau/Real-Life-B-W-Video-Colorization-Project/blob/main/ImageColorizerGANV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ads3J-VraLRG"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Image Colorization GAN\n",
        "# ----------------------\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Configuration\n",
        "# ------------------\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS = 1\n",
        "EPOCHS = 70\n",
        "BATCH_SIZE = 256\n",
        "LAMBDA = 100\n",
        "DATA_DIR = \"/content/drive/MyDrive/ImageNet\"  # Update with your path\n",
        "WORKDIR = \"/content/drive/MyDrive/Colorization\"\n",
        "CHECKPOINT_DIR = os.path.join(WORKDIR, \"checkpoints\")\n",
        "RESULTS_DIR = os.path.join(WORKDIR, \"results\")\n",
        "\n",
        "# Enable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "1jffqiuUahQR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Data Pipeline\n",
        "# ------------------\n",
        "def load_mean(data_dir):\n",
        "    \"\"\"Load mean image from first training batch\"\"\"\n",
        "    with open(os.path.join(data_dir, 'train_data_batch_1'), 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        mean = data['mean'].astype(np.float32) / 255.0\n",
        "        return mean.reshape(3, IMAGE_SIZE, IMAGE_SIZE).transpose(1, 2, 0)\n",
        "\n",
        "def data_generator(data_dir, split='train'):\n",
        "    mean = load_mean(data_dir) if split == 'train' else None\n",
        "    files = [f'train_data_batch_{i}' for i in range(1, 11)] if split == 'train' else ['val_data']\n",
        "\n",
        "    for file in files:\n",
        "        path = os.path.join(data_dir, file)\n",
        "        try:\n",
        "            with open(path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                x = data['data'].astype(np.float32) / 255.0\n",
        "                x = x.reshape(-1, 3, IMAGE_SIZE, IMAGE_SIZE).transpose(0, 2, 3, 1)\n",
        "\n",
        "                if mean is not None:\n",
        "                    x -= mean\n",
        "\n",
        "                for i in range(0, x.shape[0], BATCH_SIZE):\n",
        "                    batch_rgb = x[i:i+BATCH_SIZE]\n",
        "                    batch_lab = np.array([rgb2lab(img) for img in batch_rgb])\n",
        "                    L = batch_lab[..., 0:1].astype(np.float32)\n",
        "                    AB = (batch_lab[..., 1:] / 128.0).astype(np.float32)\n",
        "                    yield L, AB\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load {path}: {str(e)}\")\n",
        "            continue  # Skip problematic files\n",
        "\n",
        "def create_dataset(data_dir, split='train'):\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(data_dir, split),\n",
        "        output_signature=(  # ✅ Proper parentheses\n",
        "            tf.TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None, 64, 64, 2), dtype=tf.float32)\n",
        "        )\n",
        "    ).prefetch(tf.data.AUTOTUNE)  # ✅ .prefetch() called on dataset"
      ],
      "metadata": {
        "id": "oXsq8r77afW4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Model Architectures\n",
        "# ------------------\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    model = Sequential()\n",
        "    model.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                          kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    return model\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    model = Sequential()\n",
        "    model.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                    kernel_initializer=initializer, use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.ReLU())\n",
        "    return model\n",
        "\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
        "\n",
        "    # Encoder\n",
        "    d1 = downsample(64, 4, False)(inputs)    # 32x32\n",
        "    d2 = downsample(128, 4)(d1)              # 16x16\n",
        "    d3 = downsample(256, 4)(d2)              # 8x8\n",
        "    d4 = downsample(512, 4)(d3)              # 4x4\n",
        "\n",
        "    # Decoder\n",
        "    u1 = upsample(512, 4, True)(d4)          # 8x8\n",
        "    u1 = layers.Concatenate()([u1, d3])\n",
        "    u2 = upsample(256, 4)(u1)                # 16x16\n",
        "    u2 = layers.Concatenate()([u2, d2])\n",
        "    u3 = upsample(128, 4)(u2)                # 32x32\n",
        "    u3 = layers.Concatenate()([u3, d1])\n",
        "    u4 = upsample(64, 4)(u3)                 # 64x64\n",
        "\n",
        "    output = layers.Conv2D(2, 3, padding='same', activation='tanh')(u4)\n",
        "    return Model(inputs, output)\n",
        "\n",
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "\n",
        "    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "    return Model(inputs, x)"
      ],
      "metadata": {
        "id": "6YOSJkmZadOh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Setup\n",
        "# ------------------\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=generator_optimizer,\n",
        "    discriminator_optimizer=discriminator_optimizer,\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        "    epoch=tf.Variable(0)\n",
        ")\n",
        "manager = tf.train.CheckpointManager(checkpoint, CHECKPOINT_DIR, max_to_keep=3)"
      ],
      "metadata": {
        "id": "ENkU2C05abcZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Utilities\n",
        "# ------------------\n",
        "def generate_images(model, test_input, epoch):\n",
        "    input_L = test_input[0]  # ✅ Extract L channel\n",
        "    target_AB = test_input[1]  # Ground truth AB\n",
        "\n",
        "    # Predict using only L\n",
        "    prediction = model(input_L, training=False)[0].numpy()\n",
        "    L = input_L[0].numpy()[..., 0]  # Use first sample in batch\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Input (grayscale)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(L, cmap='gray')\n",
        "    plt.title(\"Input\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground truth (colorized)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    true_rgb = lab2rgb(np.dstack((L, target_AB[0].numpy() * 128)))  # ✅ Use target_AB\n",
        "    plt.imshow(true_rgb)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Predicted (colorized)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    pred_rgb = lab2rgb(np.dstack((L, prediction * 128)))\n",
        "    plt.imshow(pred_rgb)\n",
        "    plt.title(\"Predicted\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, f'epoch_{epoch+1}.png'))\n",
        "    plt.close()\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_L, input_AB):\n",
        "    # Cast to mixed precision\n",
        "    input_L = tf.cast(input_L, tf.float16)\n",
        "    input_AB = tf.cast(input_AB, tf.float16)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        generated_AB = generator(input_L, training=True)\n",
        "\n",
        "        # Create concatenated images\n",
        "        real_images = tf.concat([input_L, input_AB], axis=-1)\n",
        "        fake_images = tf.concat([input_L, generated_AB], axis=-1)\n",
        "\n",
        "        # Discriminator outputs\n",
        "        disc_real = discriminator(real_images, training=True)\n",
        "        disc_fake = discriminator(fake_images, training=True)\n",
        "\n",
        "        # Loss calculations\n",
        "        gen_loss = tf.keras.losses.binary_crossentropy(\n",
        "            tf.ones_like(disc_fake), disc_fake) + LAMBDA * tf.reduce_mean(tf.abs(input_AB - generated_AB))\n",
        "        disc_loss = tf.keras.losses.binary_crossentropy(\n",
        "            tf.ones_like(disc_real), disc_real) + tf.keras.losses.binary_crossentropy(\n",
        "            tf.zeros_like(disc_fake), disc_fake)\n",
        "\n",
        "    # Apply gradient clipping\n",
        "    gen_grads = tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gen_grads = [tf.clip_by_norm(g, 1.0) for g in gen_grads]\n",
        "    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "\n",
        "    disc_grads = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    disc_grads = [tf.clip_by_norm(g, 1.0) for g in disc_grads]\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zs6PxIdJaVn0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Training Loop\n",
        "# ------------------\n",
        "def train():\n",
        "    train_dataset = create_dataset(DATA_DIR, 'train')\n",
        "    val_dataset = create_dataset(DATA_DIR, 'val')\n",
        "\n",
        "    if manager.latest_checkpoint:\n",
        "        checkpoint.restore(manager.latest_checkpoint)\n",
        "        print(f\"Resumed from epoch {checkpoint.epoch.numpy()}\")\n",
        "\n",
        "    for epoch in range(checkpoint.epoch.numpy(), EPOCHS):\n",
        "        start = time.time()\n",
        "        gen_losses, disc_losses = [], []\n",
        "\n",
        "        for batch, (L, AB) in enumerate(train_dataset):\n",
        "            gen_loss, disc_loss = train_step(L, AB)\n",
        "            gen_losses.append(gen_loss)\n",
        "            disc_losses.append(disc_loss)\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                gen_loss_val = gen_loss.numpy().item()\n",
        "                disc_loss_val = disc_loss.numpy().item()\n",
        "                print(f\"Epoch {epoch+1} Batch {batch} | Gen: {gen_loss_val:.2f} Disc: {disc_loss_val:.2f}\")\n",
        "                tf.keras.backend.clear_session()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            manager.save()\n",
        "            test_batch = next(iter(val_dataset))\n",
        "            generate_images(generator, test_batch, epoch)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(f\"Time: {time.time()-start:.2f}s\")\n",
        "        print(f\"Gen Loss: {np.mean(gen_losses):.4f}\")\n",
        "        print(f\"Disc Loss: {np.mean(disc_losses):.4f}\\n\")\n",
        "        checkpoint.epoch.assign_add(1)"
      ],
      "metadata": {
        "id": "BOasui2daTd5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "LvKNoJXlaRZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Inference Function\n",
        "# ----------------------\n",
        "def colorize_image(model, image_path, output_path, image_size=64):\n",
        "    \"\"\"\n",
        "    Colorizes a single image using the trained generator.\n",
        "\n",
        "    Args:\n",
        "        model: Trained generator model\n",
        "        image_path: Path to input grayscale/RGB image\n",
        "        output_path: Path to save colorized image\n",
        "        image_size: Size to resize image (must match model input)\n",
        "    \"\"\"\n",
        "    # Load and preprocess image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not load image at {image_path}\")\n",
        "\n",
        "    # Convert to RGB if needed\n",
        "    if image.ndim == 2:  # Grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    else:  # BGR to RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize and normalize\n",
        "    image = cv2.resize(image, (image_size, image_size))\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Convert to LAB and extract L channel\n",
        "    lab = rgb2lab(image)\n",
        "    L = lab[:, :, 0:1]  # (H, W, 1)\n",
        "\n",
        "    # Add batch dimension and predict\n",
        "    L_batch = np.expand_dims(L, axis=0)  # (1, H, W, 1)\n",
        "    AB_pred = model.predict(L_batch, verbose=0)[0]  # (H, W, 2)\n",
        "\n",
        "    # Denormalize AB channels\n",
        "    AB_pred = (AB_pred * 128.0).astype(np.float32)\n",
        "\n",
        "    # Combine with L and convert to RGB\n",
        "    colorized_lab = np.concatenate([L, AB_pred], axis=-1)\n",
        "    colorized_rgb = lab2rgb(colorized_lab)\n",
        "\n",
        "    # Clip and save\n",
        "    colorized_rgb = np.clip(colorized_rgb, 0, 1)\n",
        "    plt.imsave(output_path, colorized_rgb)\n",
        "    print(f\"Colorized image saved to {output_path}\")"
      ],
      "metadata": {
        "id": "PmQiENqcR8bz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, use like this:\n",
        "colorize_image(\n",
        "    generator,\n",
        "    \"/content/my_grayscale_image.jpg\",  # Input path\n",
        "    \"/content/colorized_result.jpg\"     # Output path\n",
        ")"
      ],
      "metadata": {
        "id": "TfAazZuISFbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Parallel Video Colorization with Temporal Consistency\n",
        "# ----------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "# ----------------------\n",
        "# Video Colorizer with Content Directory Temp Files\n",
        "# ----------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "class VideoColorizer:\n",
        "    def __init__(self, model, temporal_alpha=0.8, blend_factor=0.7):\n",
        "        self.model = model\n",
        "        self.temporal_alpha = temporal_alpha\n",
        "        self.blend_factor = blend_factor\n",
        "        # Optical flow parameters\n",
        "        self.flow_params = {\n",
        "            'pyr_scale': 0.5,\n",
        "            'levels': 3,\n",
        "            'winsize': 15,\n",
        "            'iterations': 3,\n",
        "            'poly_n': 5,\n",
        "            'poly_sigma': 1.2,\n",
        "            'flags': cv2.OPTFLOW_FARNEBACK_GAUSSIAN\n",
        "        }\n",
        "        # Create temp directory in Colab's content directory\n",
        "        self.temp_dir = tempfile.TemporaryDirectory(\n",
        "            dir='/content',\n",
        "            prefix='colorizer_temp_'\n",
        "        )\n",
        "        os.makedirs(self.temp_dir.name, exist_ok=True)\n",
        "        os.chmod(self.temp_dir.name, 0o777)  # Ensure write permissions\n",
        "        print(f\"Created temp directory at: {self.temp_dir.name}\")\n",
        "\n",
        "    def _process_chunk(self, frames, start_idx):\n",
        "        \"\"\"Process a chunk of frames in parallel\"\"\"\n",
        "        print(f\"Processing chunk starting at {start_idx} with {len(frames)} frames\")\n",
        "\n",
        "        for local_idx, frame in enumerate(frames):\n",
        "            global_idx = start_idx + local_idx\n",
        "            save_path = os.path.join(\n",
        "                self.temp_dir.name,\n",
        "                f\"frame_{global_idx:06d}.npy\"  # Consistent naming\n",
        "            )\n",
        "\n",
        "            # Debug: Verify frame content\n",
        "            if frame is None or frame.size == 0:\n",
        "                raise ValueError(f\"Invalid frame at index {global_idx}\")\n",
        "\n",
        "            # Colorization pipeline\n",
        "            resized_rgb = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), (64, 64))\n",
        "            lab = rgb2lab(resized_rgb.astype(np.float32)/255.0)\n",
        "            L = lab[:, :, 0:1]\n",
        "            AB = self.model.predict(np.expand_dims(L, axis=0), verbose=0)[0]\n",
        "\n",
        "            # Save data with verification\n",
        "            data = {\n",
        "                'frame': frame,\n",
        "                'L': L,\n",
        "                'AB': AB,\n",
        "                'gray': cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            }\n",
        "            np.save(save_path, data)\n",
        "            print(f\"Saved frame {global_idx} to {save_path}\")\n",
        "\n",
        "        # Immediate verification of saved files\n",
        "        saved_files = [f for f in os.listdir(self.temp_dir.name)\n",
        "                      if f.startswith(f\"frame_{start_idx:06d}\")]\n",
        "        print(f\"Saved {len(saved_files)} files in this chunk\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _temporal_smooth(self, prev_data, current_data):\n",
        "        \"\"\"Apply temporal consistency between frames\"\"\"\n",
        "        if prev_data is None:\n",
        "            return current_data['AB']\n",
        "\n",
        "        # Compute optical flow at model resolution\n",
        "        target_size = (64, 64)\n",
        "        prev_gray = cv2.resize(prev_data['gray'], target_size)\n",
        "        current_gray = cv2.resize(current_data['gray'], target_size)\n",
        "\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            prev_gray, current_gray,\n",
        "            None, **self.flow_params\n",
        "        )\n",
        "\n",
        "        # Create normalized coordinate grid\n",
        "        h, w = target_size\n",
        "        x_map, y_map = np.meshgrid(np.arange(w), np.arange(h))\n",
        "        flow_map = np.stack([\n",
        "           (x_map + flow[..., 0]).astype(np.float32),\n",
        "           (y_map + flow[..., 1]).astype(np.float32)\n",
        "        ], axis=-1)\n",
        "\n",
        "        # Ensure coordinates stay within image bounds\n",
        "        flow_map[..., 0] = np.clip(flow_map[..., 0], 0, w-1)\n",
        "        flow_map[..., 1] = np.clip(flow_map[..., 1], 0, h-1)\n",
        "\n",
        "\n",
        "\n",
        "        # Warp previous AB channels\n",
        "        warped_AB = cv2.remap(\n",
        "            prev_data['AB'].astype(np.float32),  # Ensure float32 input\n",
        "            flow_map,\n",
        "            None,\n",
        "            cv2.INTER_LINEAR,\n",
        "            borderMode=cv2.BORDER_REFLECT\n",
        "        )\n",
        "\n",
        "        # Handle invalid regions (black borders from warping)\n",
        "        mask = (warped_AB == 0).all(axis=-1, keepdims=True)\n",
        "        blended_AB = np.where(mask, current_data['AB'],\n",
        "                         self.blend_factor * current_data['AB'] +\n",
        "                         (1 - self.blend_factor) * warped_AB)\n",
        "\n",
        "        smoothed_AB = self.temporal_alpha * blended_AB + \\\n",
        "                     (1 - self.temporal_alpha) * warped_AB\n",
        "        return smoothed_AB\n",
        "\n",
        "    def colorize_video(self, input_path, output_path,\n",
        "                     batch_size=16, workers=8,\n",
        "                     frame_limit=None):\n",
        "        # Initialization checks\n",
        "        print(f\"Initializing video processing\")\n",
        "        print(f\"Input: {input_path}\")\n",
        "        print(f\"Output: {output_path}\")\n",
        "        # Verify input video exists\n",
        "        if not os.path.exists(input_path):\n",
        "            raise FileNotFoundError(f\"Input video {input_path} not found\")\n",
        "\n",
        "        # Verify video can be read\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Couldn't open video {input_path} - check codec support\")\n",
        "\n",
        "        \"\"\"Main processing pipeline\"\"\"\n",
        "        # Phase 1: Parallel colorization\n",
        "        print(f\"Temp directory contains: {os.listdir(self.temp_dir.name)} (initial)\")\n",
        "        if not os.path.exists(self.temp_dir.name):\n",
        "            raise FileNotFoundError(\"Temp directory not created!\")\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            print(\"❌ Failed to open input video\")\n",
        "            return\n",
        "\n",
        "        executor = ThreadPoolExecutor(max_workers=workers)\n",
        "        futures = []\n",
        "        frame_count = 0\n",
        "        print(f\"Total frames: {frame_count}\")\n",
        "        print(f\"FPS: {cap.get(cv2.CAP_PROP_FPS)}\")\n",
        "        print(f\"Resolution: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}x{cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}\")\n",
        "        chunk = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or (frame_limit and frame_count >= frame_limit):\n",
        "                break\n",
        "\n",
        "            chunk.append(frame)\n",
        "            frame_count += 1\n",
        "\n",
        "            if len(chunk) == batch_size:\n",
        "                future = executor.submit(self._process_chunk, chunk.copy(), frame_count - batch_size)\n",
        "                futures.append(future)\n",
        "                chunk = []\n",
        "\n",
        "        # Process remaining frames\n",
        "        if len(chunk) > 0:\n",
        "            future = executor.submit(self._process_chunk, chunk, frame_count - len(chunk))\n",
        "            futures.append(future)\n",
        "\n",
        "        # Wait for all chunks to complete\n",
        "        _ = [f.result() for f in futures]\n",
        "        cap.release()\n",
        "\n",
        "        # Phase 2: Temporal smoothing and output\n",
        "\n",
        "        writer = cv2.VideoWriter(output_path,\n",
        "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                        cap.get(cv2.CAP_PROP_FPS),\n",
        "                        (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "        prev_data = None\n",
        "        saved_files = sorted(\n",
        "        [f for f in os.listdir(self.temp_dir.name)\n",
        "        if f.startswith('frame_') and f.endswith('.npy')],\n",
        "        key=lambda x: int(x.split('_')[1].split('.')[0])\n",
        "        )\n",
        "\n",
        "        print(f\"Found {len(saved_files)} processed frames\")\n",
        "        print(\"First 5 files:\", saved_files[:5])\n",
        "\n",
        "        if not saved_files:\n",
        "            raise RuntimeError(\"No processed frames found in temp directory\")\n",
        "\n",
        "        # Process frames using ACTUAL saved filenames\n",
        "        for filename in saved_files:\n",
        "            file_path = os.path.join(self.temp_dir.name, filename)\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"Missing frame file: {file_path}\")\n",
        "\n",
        "        data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "        # Apply temporal smoothing\n",
        "        smoothed_AB = self._temporal_smooth(prev_data, data)\n",
        "\n",
        "        # Generate final frame\n",
        "        # Generate final frame\n",
        "        rgb_resized = cv2.resize(\n",
        "                  cv2.cvtColor(data['frame'], cv2.COLOR_BGR2RGB),\n",
        "                  (64, 64)  # Explicit dsize parameter\n",
        "                  )\n",
        "        lab = rgb2lab(rgb_resized.astype(np.float32)/255.0)\n",
        "        final_lab = np.concatenate([lab[..., 0:1], smoothed_AB], axis=-1)\n",
        "\n",
        "        # Resize back to original dimensions\n",
        "        rgb = (lab2rgb(final_lab) * 255).astype(np.uint8)\n",
        "        final_frame = cv2.resize(rgb, (data['frame'].shape[1], data['frame'].shape[0]))\n",
        "\n",
        "\n",
        "        writer.write(final_frame)\n",
        "        prev_data = {'AB': smoothed_AB, 'gray': data['gray']}\n",
        "\n",
        "        writer.release()\n",
        "        self.temp_dir.cleanup()\n"
      ],
      "metadata": {
        "id": "6FYBJCluSoP8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colorizer = VideoColorizer(generator, temporal_alpha=0.85)\n",
        "\n",
        "\n",
        "colorizer.colorize_video(\n",
        "    \"/content/Video/input.mp4\",\n",
        "    \"/content/Video/output.mp4\",\n",
        "    batch_size=32,\n",
        "    workers=8,\n",
        "    frame_limit=300\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiFkW4nLYBbb",
        "outputId": "d08ae7ca-7b79-472c-cfec-5f40298b1fc3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created temp directory at: /content/colorizer_temp_d1y7vtph\n",
            "Initializing video processing\n",
            "Input: /content/Video/input.mp4\n",
            "Output: /content/Video/output.mp4\n",
            "Temp directory contains: [] (initial)\n",
            "Total frames: 0\n",
            "FPS: 29.97002997002997\n",
            "Resolution: 1280.0x720.0\n",
            "Processing chunk starting at 0 with 32 frames\n",
            "Saved frame 0 to /content/colorizer_temp_d1y7vtph/frame_000000.npy\n",
            "Processing chunk starting at 32 with 32 frames\n",
            "Saved frame 1 to /content/colorizer_temp_d1y7vtph/frame_000001.npy\n",
            "Processing chunk starting at 64 with 32 frames\n",
            "Saved frame 32 to /content/colorizer_temp_d1y7vtph/frame_000032.npy\n",
            "Saved frame 64 to /content/colorizer_temp_d1y7vtph/frame_000064.npy\n",
            "Saved frame 33 to /content/colorizer_temp_d1y7vtph/frame_000033.npy\n",
            "Processing chunk starting at 96 with 32 frames\n",
            "Saved frame 2 to /content/colorizer_temp_d1y7vtph/frame_000002.npy\n",
            "Saved frame 65 to /content/colorizer_temp_d1y7vtph/frame_000065.npy\n",
            "Saved frame 34 to /content/colorizer_temp_d1y7vtph/frame_000034.npy\n",
            "Saved frame 96 to /content/colorizer_temp_d1y7vtph/frame_000096.npy\n",
            "Saved frame 3 to /content/colorizer_temp_d1y7vtph/frame_000003.npy\n",
            "Processing chunk starting at 128 with 32 frames\n",
            "Saved frame 66 to /content/colorizer_temp_d1y7vtph/frame_000066.npy\n",
            "Saved frame 35 to /content/colorizer_temp_d1y7vtph/frame_000035.npy\n",
            "Saved frame 97 to /content/colorizer_temp_d1y7vtph/frame_000097.npy\n",
            "Saved frame 128 to /content/colorizer_temp_d1y7vtph/frame_000128.npy\n",
            "Saved frame 4 to /content/colorizer_temp_d1y7vtph/frame_000004.npy\n",
            "Processing chunk starting at 160 with 32 frames\n",
            "Saved frame 36 to /content/colorizer_temp_d1y7vtph/frame_000036.npy\n",
            "Saved frame 67 to /content/colorizer_temp_d1y7vtph/frame_000067.npy\n",
            "Saved frame 98 to /content/colorizer_temp_d1y7vtph/frame_000098.npy\n",
            "Saved frame 129 to /content/colorizer_temp_d1y7vtph/frame_000129.npy\n",
            "Saved frame 5 to /content/colorizer_temp_d1y7vtph/frame_000005.npy\n",
            "Saved frame 160 to /content/colorizer_temp_d1y7vtph/frame_000160.npy\n",
            "Processing chunk starting at 192 with 32 frames\n",
            "Saved frame 37 to /content/colorizer_temp_d1y7vtph/frame_000037.npy\n",
            "Saved frame 68 to /content/colorizer_temp_d1y7vtph/frame_000068.npy\n",
            "Saved frame 99 to /content/colorizer_temp_d1y7vtph/frame_000099.npy\n",
            "Saved frame 130 to /content/colorizer_temp_d1y7vtph/frame_000130.npy\n",
            "Saved frame 161 to /content/colorizer_temp_d1y7vtph/frame_000161.npy\n",
            "Saved frame 6 to /content/colorizer_temp_d1y7vtph/frame_000006.npy\n",
            "Processing chunk starting at 224 with 32 frames\n",
            "Saved frame 38 to /content/colorizer_temp_d1y7vtph/frame_000038.npy\n",
            "Saved frame 192 to /content/colorizer_temp_d1y7vtph/frame_000192.npy\n",
            "Saved frame 69 to /content/colorizer_temp_d1y7vtph/frame_000069.npy\n",
            "Saved frame 100 to /content/colorizer_temp_d1y7vtph/frame_000100.npy\n",
            "Saved frame 162 to /content/colorizer_temp_d1y7vtph/frame_000162.npy\n",
            "Saved frame 131 to /content/colorizer_temp_d1y7vtph/frame_000131.npy\n",
            "Saved frame 7 to /content/colorizer_temp_d1y7vtph/frame_000007.npy\n",
            "Saved frame 224 to /content/colorizer_temp_d1y7vtph/frame_000224.npySaved frame 39 to /content/colorizer_temp_d1y7vtph/frame_000039.npy\n",
            "\n",
            "Saved frame 193 to /content/colorizer_temp_d1y7vtph/frame_000193.npy\n",
            "Saved frame 70 to /content/colorizer_temp_d1y7vtph/frame_000070.npy\n",
            "Saved frame 101 to /content/colorizer_temp_d1y7vtph/frame_000101.npy\n",
            "Saved frame 163 to /content/colorizer_temp_d1y7vtph/frame_000163.npy\n",
            "Saved frame 132 to /content/colorizer_temp_d1y7vtph/frame_000132.npy\n",
            "Saved frame 8 to /content/colorizer_temp_d1y7vtph/frame_000008.npy\n",
            "Saved frame 40 to /content/colorizer_temp_d1y7vtph/frame_000040.npy\n",
            "Saved frame 225 to /content/colorizer_temp_d1y7vtph/frame_000225.npy\n",
            "Saved frame 194 to /content/colorizer_temp_d1y7vtph/frame_000194.npy\n",
            "Saved frame 71 to /content/colorizer_temp_d1y7vtph/frame_000071.npy\n",
            "Saved frame 133 to /content/colorizer_temp_d1y7vtph/frame_000133.npy\n",
            "Saved frame 102 to /content/colorizer_temp_d1y7vtph/frame_000102.npy\n",
            "Saved frame 164 to /content/colorizer_temp_d1y7vtph/frame_000164.npy\n",
            "Saved frame 9 to /content/colorizer_temp_d1y7vtph/frame_000009.npy\n",
            "Saved frame 41 to /content/colorizer_temp_d1y7vtph/frame_000041.npy\n",
            "Saved frame 226 to /content/colorizer_temp_d1y7vtph/frame_000226.npy\n",
            "Saved frame 195 to /content/colorizer_temp_d1y7vtph/frame_000195.npy\n",
            "Saved frame 134 to /content/colorizer_temp_d1y7vtph/frame_000134.npy\n",
            "Saved frame 103 to /content/colorizer_temp_d1y7vtph/frame_000103.npy\n",
            "Saved frame 72 to /content/colorizer_temp_d1y7vtph/frame_000072.npy\n",
            "Saved frame 165 to /content/colorizer_temp_d1y7vtph/frame_000165.npy\n",
            "Saved frame 10 to /content/colorizer_temp_d1y7vtph/frame_000010.npy\n",
            "Saved frame 42 to /content/colorizer_temp_d1y7vtph/frame_000042.npy\n",
            "Saved frame 227 to /content/colorizer_temp_d1y7vtph/frame_000227.npy\n",
            "Saved frame 196 to /content/colorizer_temp_d1y7vtph/frame_000196.npy\n",
            "Saved frame 73 to /content/colorizer_temp_d1y7vtph/frame_000073.npy\n",
            "Saved frame 135 to /content/colorizer_temp_d1y7vtph/frame_000135.npy\n",
            "Saved frame 166 to /content/colorizer_temp_d1y7vtph/frame_000166.npy\n",
            "Saved frame 104 to /content/colorizer_temp_d1y7vtph/frame_000104.npy\n",
            "Saved frame 11 to /content/colorizer_temp_d1y7vtph/frame_000011.npy\n",
            "Saved frame 43 to /content/colorizer_temp_d1y7vtph/frame_000043.npy\n",
            "Saved frame 197 to /content/colorizer_temp_d1y7vtph/frame_000197.npy\n",
            "Saved frame 228 to /content/colorizer_temp_d1y7vtph/frame_000228.npy\n",
            "Saved frame 74 to /content/colorizer_temp_d1y7vtph/frame_000074.npy\n",
            "Saved frame 105 to /content/colorizer_temp_d1y7vtph/frame_000105.npy\n",
            "Saved frame 136 to /content/colorizer_temp_d1y7vtph/frame_000136.npy\n",
            "Saved frame 167 to /content/colorizer_temp_d1y7vtph/frame_000167.npy\n",
            "Saved frame 12 to /content/colorizer_temp_d1y7vtph/frame_000012.npy\n",
            "Saved frame 44 to /content/colorizer_temp_d1y7vtph/frame_000044.npy\n",
            "Saved frame 229 to /content/colorizer_temp_d1y7vtph/frame_000229.npy\n",
            "Saved frame 198 to /content/colorizer_temp_d1y7vtph/frame_000198.npy\n",
            "Saved frame 75 to /content/colorizer_temp_d1y7vtph/frame_000075.npy\n",
            "Saved frame 137 to /content/colorizer_temp_d1y7vtph/frame_000137.npy\n",
            "Saved frame 106 to /content/colorizer_temp_d1y7vtph/frame_000106.npy\n",
            "Saved frame 168 to /content/colorizer_temp_d1y7vtph/frame_000168.npy\n",
            "Saved frame 13 to /content/colorizer_temp_d1y7vtph/frame_000013.npy\n",
            "Saved frame 45 to /content/colorizer_temp_d1y7vtph/frame_000045.npy\n",
            "Saved frame 199 to /content/colorizer_temp_d1y7vtph/frame_000199.npy\n",
            "Saved frame 230 to /content/colorizer_temp_d1y7vtph/frame_000230.npy\n",
            "Saved frame 107 to /content/colorizer_temp_d1y7vtph/frame_000107.npy\n",
            "Saved frame 138 to /content/colorizer_temp_d1y7vtph/frame_000138.npy\n",
            "Saved frame 14 to /content/colorizer_temp_d1y7vtph/frame_000014.npy\n",
            "Saved frame 169 to /content/colorizer_temp_d1y7vtph/frame_000169.npy\n",
            "Saved frame 76 to /content/colorizer_temp_d1y7vtph/frame_000076.npy\n",
            "Saved frame 46 to /content/colorizer_temp_d1y7vtph/frame_000046.npy\n",
            "Saved frame 200 to /content/colorizer_temp_d1y7vtph/frame_000200.npy\n",
            "Saved frame 231 to /content/colorizer_temp_d1y7vtph/frame_000231.npy\n",
            "Saved frame 108 to /content/colorizer_temp_d1y7vtph/frame_000108.npy\n",
            "Saved frame 15 to /content/colorizer_temp_d1y7vtph/frame_000015.npy\n",
            "Saved frame 139 to /content/colorizer_temp_d1y7vtph/frame_000139.npy\n",
            "Saved frame 170 to /content/colorizer_temp_d1y7vtph/frame_000170.npy\n",
            "Saved frame 77 to /content/colorizer_temp_d1y7vtph/frame_000077.npy\n",
            "Saved frame 201 to /content/colorizer_temp_d1y7vtph/frame_000201.npy\n",
            "Saved frame 47 to /content/colorizer_temp_d1y7vtph/frame_000047.npy\n",
            "Saved frame 232 to /content/colorizer_temp_d1y7vtph/frame_000232.npy\n",
            "Saved frame 109 to /content/colorizer_temp_d1y7vtph/frame_000109.npy\n",
            "Saved frame 16 to /content/colorizer_temp_d1y7vtph/frame_000016.npy\n",
            "Saved frame 171 to /content/colorizer_temp_d1y7vtph/frame_000171.npy\n",
            "Saved frame 140 to /content/colorizer_temp_d1y7vtph/frame_000140.npy\n",
            "Saved frame 78 to /content/colorizer_temp_d1y7vtph/frame_000078.npy\n",
            "Saved frame 202 to /content/colorizer_temp_d1y7vtph/frame_000202.npy\n",
            "Saved frame 233 to /content/colorizer_temp_d1y7vtph/frame_000233.npy\n",
            "Saved frame 48 to /content/colorizer_temp_d1y7vtph/frame_000048.npy\n",
            "Saved frame 110 to /content/colorizer_temp_d1y7vtph/frame_000110.npy\n",
            "Saved frame 17 to /content/colorizer_temp_d1y7vtph/frame_000017.npy\n",
            "Saved frame 141 to /content/colorizer_temp_d1y7vtph/frame_000141.npy\n",
            "Saved frame 79 to /content/colorizer_temp_d1y7vtph/frame_000079.npy\n",
            "Saved frame 172 to /content/colorizer_temp_d1y7vtph/frame_000172.npy\n",
            "Saved frame 203 to /content/colorizer_temp_d1y7vtph/frame_000203.npy\n",
            "Saved frame 49 to /content/colorizer_temp_d1y7vtph/frame_000049.npy\n",
            "Saved frame 111 to /content/colorizer_temp_d1y7vtph/frame_000111.npy\n",
            "Saved frame 234 to /content/colorizer_temp_d1y7vtph/frame_000234.npy\n",
            "Saved frame 18 to /content/colorizer_temp_d1y7vtph/frame_000018.npy\n",
            "Saved frame 142 to /content/colorizer_temp_d1y7vtph/frame_000142.npy\n",
            "Saved frame 80 to /content/colorizer_temp_d1y7vtph/frame_000080.npy\n",
            "Saved frame 204 to /content/colorizer_temp_d1y7vtph/frame_000204.npy\n",
            "Saved frame 173 to /content/colorizer_temp_d1y7vtph/frame_000173.npy\n",
            "Saved frame 112 to /content/colorizer_temp_d1y7vtph/frame_000112.npy\n",
            "Saved frame 50 to /content/colorizer_temp_d1y7vtph/frame_000050.npy\n",
            "Saved frame 235 to /content/colorizer_temp_d1y7vtph/frame_000235.npy\n",
            "Saved frame 19 to /content/colorizer_temp_d1y7vtph/frame_000019.npy\n",
            "Saved frame 81 to /content/colorizer_temp_d1y7vtph/frame_000081.npy\n",
            "Saved frame 143 to /content/colorizer_temp_d1y7vtph/frame_000143.npy\n",
            "Saved frame 205 to /content/colorizer_temp_d1y7vtph/frame_000205.npy\n",
            "Saved frame 174 to /content/colorizer_temp_d1y7vtph/frame_000174.npy\n",
            "Saved frame 113 to /content/colorizer_temp_d1y7vtph/frame_000113.npy\n",
            "Saved frame 51 to /content/colorizer_temp_d1y7vtph/frame_000051.npy\n",
            "Saved frame 236 to /content/colorizer_temp_d1y7vtph/frame_000236.npy\n",
            "Saved frame 20 to /content/colorizer_temp_d1y7vtph/frame_000020.npy\n",
            "Saved frame 82 to /content/colorizer_temp_d1y7vtph/frame_000082.npy\n",
            "Saved frame 144 to /content/colorizer_temp_d1y7vtph/frame_000144.npy\n",
            "Saved frame 206 to /content/colorizer_temp_d1y7vtph/frame_000206.npy\n",
            "Saved frame 114 to /content/colorizer_temp_d1y7vtph/frame_000114.npy\n",
            "Saved frame 175 to /content/colorizer_temp_d1y7vtph/frame_000175.npy\n",
            "Saved frame 52 to /content/colorizer_temp_d1y7vtph/frame_000052.npy\n",
            "Saved frame 237 to /content/colorizer_temp_d1y7vtph/frame_000237.npy\n",
            "Saved frame 21 to /content/colorizer_temp_d1y7vtph/frame_000021.npy\n",
            "Saved frame 83 to /content/colorizer_temp_d1y7vtph/frame_000083.npy\n",
            "Saved frame 145 to /content/colorizer_temp_d1y7vtph/frame_000145.npy\n",
            "Saved frame 207 to /content/colorizer_temp_d1y7vtph/frame_000207.npy\n",
            "Saved frame 176 to /content/colorizer_temp_d1y7vtph/frame_000176.npy\n",
            "Saved frame 53 to /content/colorizer_temp_d1y7vtph/frame_000053.npy\n",
            "Saved frame 238 to /content/colorizer_temp_d1y7vtph/frame_000238.npy\n",
            "Saved frame 115 to /content/colorizer_temp_d1y7vtph/frame_000115.npy\n",
            "Saved frame 22 to /content/colorizer_temp_d1y7vtph/frame_000022.npy\n",
            "Saved frame 84 to /content/colorizer_temp_d1y7vtph/frame_000084.npy\n",
            "Saved frame 208 to /content/colorizer_temp_d1y7vtph/frame_000208.npy\n",
            "Saved frame 177 to /content/colorizer_temp_d1y7vtph/frame_000177.npy\n",
            "Saved frame 146 to /content/colorizer_temp_d1y7vtph/frame_000146.npy\n",
            "Saved frame 239 to /content/colorizer_temp_d1y7vtph/frame_000239.npy\n",
            "Saved frame 116 to /content/colorizer_temp_d1y7vtph/frame_000116.npy\n",
            "Saved frame 54 to /content/colorizer_temp_d1y7vtph/frame_000054.npy\n",
            "Saved frame 23 to /content/colorizer_temp_d1y7vtph/frame_000023.npy\n",
            "Saved frame 85 to /content/colorizer_temp_d1y7vtph/frame_000085.npy\n",
            "Saved frame 209 to /content/colorizer_temp_d1y7vtph/frame_000209.npy\n",
            "Saved frame 147 to /content/colorizer_temp_d1y7vtph/frame_000147.npy\n",
            "Saved frame 178 to /content/colorizer_temp_d1y7vtph/frame_000178.npy\n",
            "Saved frame 117 to /content/colorizer_temp_d1y7vtph/frame_000117.npy\n",
            "Saved frame 55 to /content/colorizer_temp_d1y7vtph/frame_000055.npy\n",
            "Saved frame 240 to /content/colorizer_temp_d1y7vtph/frame_000240.npy\n",
            "Saved frame 86 to /content/colorizer_temp_d1y7vtph/frame_000086.npy\n",
            "Saved frame 24 to /content/colorizer_temp_d1y7vtph/frame_000024.npy\n",
            "Saved frame 210 to /content/colorizer_temp_d1y7vtph/frame_000210.npy\n",
            "Saved frame 179 to /content/colorizer_temp_d1y7vtph/frame_000179.npy\n",
            "Saved frame 148 to /content/colorizer_temp_d1y7vtph/frame_000148.npy\n",
            "Saved frame 56 to /content/colorizer_temp_d1y7vtph/frame_000056.npy\n",
            "Saved frame 241 to /content/colorizer_temp_d1y7vtph/frame_000241.npy\n",
            "Saved frame 118 to /content/colorizer_temp_d1y7vtph/frame_000118.npy\n",
            "Saved frame 211 to /content/colorizer_temp_d1y7vtph/frame_000211.npy\n",
            "Saved frame 25 to /content/colorizer_temp_d1y7vtph/frame_000025.npy\n",
            "Saved frame 87 to /content/colorizer_temp_d1y7vtph/frame_000087.npy\n",
            "Saved frame 180 to /content/colorizer_temp_d1y7vtph/frame_000180.npy\n",
            "Saved frame 57 to /content/colorizer_temp_d1y7vtph/frame_000057.npy\n",
            "Saved frame 149 to /content/colorizer_temp_d1y7vtph/frame_000149.npy\n",
            "Saved frame 242 to /content/colorizer_temp_d1y7vtph/frame_000242.npy\n",
            "Saved frame 119 to /content/colorizer_temp_d1y7vtph/frame_000119.npy\n",
            "Saved frame 26 to /content/colorizer_temp_d1y7vtph/frame_000026.npy\n",
            "Saved frame 212 to /content/colorizer_temp_d1y7vtph/frame_000212.npy\n",
            "Saved frame 181 to /content/colorizer_temp_d1y7vtph/frame_000181.npy\n",
            "Saved frame 88 to /content/colorizer_temp_d1y7vtph/frame_000088.npy\n",
            "Saved frame 58 to /content/colorizer_temp_d1y7vtph/frame_000058.npy\n",
            "Saved frame 150 to /content/colorizer_temp_d1y7vtph/frame_000150.npy\n",
            "Saved frame 120 to /content/colorizer_temp_d1y7vtph/frame_000120.npy\n",
            "Saved frame 243 to /content/colorizer_temp_d1y7vtph/frame_000243.npy\n",
            "Saved frame 27 to /content/colorizer_temp_d1y7vtph/frame_000027.npy\n",
            "Saved frame 213 to /content/colorizer_temp_d1y7vtph/frame_000213.npySaved frame 182 to /content/colorizer_temp_d1y7vtph/frame_000182.npy\n",
            "\n",
            "Saved frame 89 to /content/colorizer_temp_d1y7vtph/frame_000089.npy\n",
            "Saved frame 151 to /content/colorizer_temp_d1y7vtph/frame_000151.npy\n",
            "Saved frame 59 to /content/colorizer_temp_d1y7vtph/frame_000059.npy\n",
            "Saved frame 244 to /content/colorizer_temp_d1y7vtph/frame_000244.npy\n",
            "Saved frame 121 to /content/colorizer_temp_d1y7vtph/frame_000121.npy\n",
            "Saved frame 28 to /content/colorizer_temp_d1y7vtph/frame_000028.npy\n",
            "Saved frame 183 to /content/colorizer_temp_d1y7vtph/frame_000183.npy\n",
            "Saved frame 214 to /content/colorizer_temp_d1y7vtph/frame_000214.npy\n",
            "Saved frame 90 to /content/colorizer_temp_d1y7vtph/frame_000090.npy\n",
            "Saved frame 60 to /content/colorizer_temp_d1y7vtph/frame_000060.npy\n",
            "Saved frame 152 to /content/colorizer_temp_d1y7vtph/frame_000152.npy\n",
            "Saved frame 245 to /content/colorizer_temp_d1y7vtph/frame_000245.npy\n",
            "Saved frame 122 to /content/colorizer_temp_d1y7vtph/frame_000122.npy\n",
            "Saved frame 29 to /content/colorizer_temp_d1y7vtph/frame_000029.npy\n",
            "Saved frame 184 to /content/colorizer_temp_d1y7vtph/frame_000184.npy\n",
            "Saved frame 215 to /content/colorizer_temp_d1y7vtph/frame_000215.npy\n",
            "Saved frame 91 to /content/colorizer_temp_d1y7vtph/frame_000091.npy\n",
            "Saved frame 153 to /content/colorizer_temp_d1y7vtph/frame_000153.npy\n",
            "Saved frame 61 to /content/colorizer_temp_d1y7vtph/frame_000061.npy\n",
            "Saved frame 246 to /content/colorizer_temp_d1y7vtph/frame_000246.npy\n",
            "Saved frame 30 to /content/colorizer_temp_d1y7vtph/frame_000030.npy\n",
            "Saved frame 123 to /content/colorizer_temp_d1y7vtph/frame_000123.npy\n",
            "Saved frame 216 to /content/colorizer_temp_d1y7vtph/frame_000216.npy\n",
            "Saved frame 185 to /content/colorizer_temp_d1y7vtph/frame_000185.npy\n",
            "Saved frame 92 to /content/colorizer_temp_d1y7vtph/frame_000092.npy\n",
            "Saved frame 154 to /content/colorizer_temp_d1y7vtph/frame_000154.npy\n",
            "Saved frame 62 to /content/colorizer_temp_d1y7vtph/frame_000062.npy\n",
            "Saved frame 247 to /content/colorizer_temp_d1y7vtph/frame_000247.npy\n",
            "Saved frame 124 to /content/colorizer_temp_d1y7vtph/frame_000124.npy\n",
            "Saved frame 31 to /content/colorizer_temp_d1y7vtph/frame_000031.npy\n",
            "Saved 1 files in this chunk\n",
            "Processing chunk starting at 256 with 32 frames\n",
            "Saved frame 217 to /content/colorizer_temp_d1y7vtph/frame_000217.npy\n",
            "Saved frame 186 to /content/colorizer_temp_d1y7vtph/frame_000186.npy\n",
            "Saved frame 93 to /content/colorizer_temp_d1y7vtph/frame_000093.npy\n",
            "Saved frame 155 to /content/colorizer_temp_d1y7vtph/frame_000155.npy\n",
            "Saved frame 63 to /content/colorizer_temp_d1y7vtph/frame_000063.npy\n",
            "Saved 1 files in this chunk\n",
            "Processing chunk starting at 288 with 12 frames\n",
            "Saved frame 125 to /content/colorizer_temp_d1y7vtph/frame_000125.npy\n",
            "Saved frame 248 to /content/colorizer_temp_d1y7vtph/frame_000248.npy\n",
            "Saved frame 256 to /content/colorizer_temp_d1y7vtph/frame_000256.npy\n",
            "Saved frame 187 to /content/colorizer_temp_d1y7vtph/frame_000187.npy\n",
            "Saved frame 218 to /content/colorizer_temp_d1y7vtph/frame_000218.npy\n",
            "Saved frame 94 to /content/colorizer_temp_d1y7vtph/frame_000094.npy\n",
            "Saved frame 156 to /content/colorizer_temp_d1y7vtph/frame_000156.npy\n",
            "Saved frame 288 to /content/colorizer_temp_d1y7vtph/frame_000288.npy\n",
            "Saved frame 126 to /content/colorizer_temp_d1y7vtph/frame_000126.npy\n",
            "Saved frame 249 to /content/colorizer_temp_d1y7vtph/frame_000249.npy\n",
            "Saved frame 257 to /content/colorizer_temp_d1y7vtph/frame_000257.npy\n",
            "Saved frame 188 to /content/colorizer_temp_d1y7vtph/frame_000188.npy\n",
            "Saved frame 95 to /content/colorizer_temp_d1y7vtph/frame_000095.npy\n",
            "Saved frame 219 to /content/colorizer_temp_d1y7vtph/frame_000219.npy\n",
            "Saved 1 files in this chunk\n",
            "Saved frame 157 to /content/colorizer_temp_d1y7vtph/frame_000157.npy\n",
            "Saved frame 289 to /content/colorizer_temp_d1y7vtph/frame_000289.npy\n",
            "Saved frame 127 to /content/colorizer_temp_d1y7vtph/frame_000127.npy\n",
            "Saved frame 250 to /content/colorizer_temp_d1y7vtph/frame_000250.npy\n",
            "Saved 1 files in this chunk\n",
            "Saved frame 258 to /content/colorizer_temp_d1y7vtph/frame_000258.npy\n",
            "Saved frame 189 to /content/colorizer_temp_d1y7vtph/frame_000189.npy\n",
            "Saved frame 220 to /content/colorizer_temp_d1y7vtph/frame_000220.npy\n",
            "Saved frame 290 to /content/colorizer_temp_d1y7vtph/frame_000290.npy\n",
            "Saved frame 158 to /content/colorizer_temp_d1y7vtph/frame_000158.npy\n",
            "Saved frame 251 to /content/colorizer_temp_d1y7vtph/frame_000251.npy\n",
            "Saved frame 259 to /content/colorizer_temp_d1y7vtph/frame_000259.npy\n",
            "Saved frame 221 to /content/colorizer_temp_d1y7vtph/frame_000221.npy\n",
            "Saved frame 190 to /content/colorizer_temp_d1y7vtph/frame_000190.npy\n",
            "Saved frame 291 to /content/colorizer_temp_d1y7vtph/frame_000291.npy\n",
            "Saved frame 159 to /content/colorizer_temp_d1y7vtph/frame_000159.npy\n",
            "Saved 1 files in this chunk\n",
            "Saved frame 252 to /content/colorizer_temp_d1y7vtph/frame_000252.npy\n",
            "Saved frame 260 to /content/colorizer_temp_d1y7vtph/frame_000260.npy\n",
            "Saved frame 222 to /content/colorizer_temp_d1y7vtph/frame_000222.npy\n",
            "Saved frame 191 to /content/colorizer_temp_d1y7vtph/frame_000191.npy\n",
            "Saved 1 files in this chunk\n",
            "Saved frame 292 to /content/colorizer_temp_d1y7vtph/frame_000292.npy\n",
            "Saved frame 253 to /content/colorizer_temp_d1y7vtph/frame_000253.npy\n",
            "Saved frame 261 to /content/colorizer_temp_d1y7vtph/frame_000261.npy\n",
            "Saved frame 223 to /content/colorizer_temp_d1y7vtph/frame_000223.npy\n",
            "Saved 1 files in this chunk\n",
            "Saved frame 293 to /content/colorizer_temp_d1y7vtph/frame_000293.npy\n",
            "Saved frame 254 to /content/colorizer_temp_d1y7vtph/frame_000254.npy\n",
            "Saved frame 262 to /content/colorizer_temp_d1y7vtph/frame_000262.npy\n",
            "Saved frame 294 to /content/colorizer_temp_d1y7vtph/frame_000294.npy\n",
            "Saved frame 255 to /content/colorizer_temp_d1y7vtph/frame_000255.npy\n",
            "Saved 1 files in this chunk\n",
            "Saved frame 263 to /content/colorizer_temp_d1y7vtph/frame_000263.npy\n",
            "Saved frame 295 to /content/colorizer_temp_d1y7vtph/frame_000295.npy\n",
            "Saved frame 264 to /content/colorizer_temp_d1y7vtph/frame_000264.npy\n",
            "Saved frame 296 to /content/colorizer_temp_d1y7vtph/frame_000296.npy\n",
            "Saved frame 265 to /content/colorizer_temp_d1y7vtph/frame_000265.npy\n",
            "Saved frame 297 to /content/colorizer_temp_d1y7vtph/frame_000297.npy\n",
            "Saved frame 266 to /content/colorizer_temp_d1y7vtph/frame_000266.npy\n",
            "Saved frame 298 to /content/colorizer_temp_d1y7vtph/frame_000298.npy\n",
            "Saved frame 267 to /content/colorizer_temp_d1y7vtph/frame_000267.npy\n",
            "Saved frame 299 to /content/colorizer_temp_d1y7vtph/frame_000299.npy\n",
            "Saved 1 files in this chunk\n",
            "Saved frame 268 to /content/colorizer_temp_d1y7vtph/frame_000268.npy\n",
            "Saved frame 269 to /content/colorizer_temp_d1y7vtph/frame_000269.npy\n",
            "Saved frame 270 to /content/colorizer_temp_d1y7vtph/frame_000270.npy\n",
            "Saved frame 271 to /content/colorizer_temp_d1y7vtph/frame_000271.npy\n",
            "Saved frame 272 to /content/colorizer_temp_d1y7vtph/frame_000272.npy\n",
            "Saved frame 273 to /content/colorizer_temp_d1y7vtph/frame_000273.npy\n",
            "Saved frame 274 to /content/colorizer_temp_d1y7vtph/frame_000274.npy\n",
            "Saved frame 275 to /content/colorizer_temp_d1y7vtph/frame_000275.npy\n",
            "Saved frame 276 to /content/colorizer_temp_d1y7vtph/frame_000276.npy\n",
            "Saved frame 277 to /content/colorizer_temp_d1y7vtph/frame_000277.npy\n",
            "Saved frame 278 to /content/colorizer_temp_d1y7vtph/frame_000278.npy\n",
            "Saved frame 279 to /content/colorizer_temp_d1y7vtph/frame_000279.npy\n",
            "Saved frame 280 to /content/colorizer_temp_d1y7vtph/frame_000280.npy\n",
            "Saved frame 281 to /content/colorizer_temp_d1y7vtph/frame_000281.npy\n",
            "Saved frame 282 to /content/colorizer_temp_d1y7vtph/frame_000282.npy\n",
            "Saved frame 283 to /content/colorizer_temp_d1y7vtph/frame_000283.npy\n",
            "Saved frame 284 to /content/colorizer_temp_d1y7vtph/frame_000284.npy\n",
            "Saved frame 285 to /content/colorizer_temp_d1y7vtph/frame_000285.npy\n",
            "Saved frame 286 to /content/colorizer_temp_d1y7vtph/frame_000286.npy\n",
            "Saved frame 287 to /content/colorizer_temp_d1y7vtph/frame_000287.npy\n",
            "Saved 1 files in this chunk\n",
            "Found 300 processed frames\n",
            "First 5 files: ['frame_000000.npy', 'frame_000001.npy', 'frame_000002.npy', 'frame_000003.npy', 'frame_000004.npy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tempfile.TemporaryDirectory(\n",
        "            dir='/content',\n",
        "            prefix='colorizer_temp_'\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXS8Ko74u4qZ",
        "outputId": "1166857b-ee21-4a77-da27-47f51ac47f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TemporaryDirectory '/content/colorizer_temp_9sxw912w'>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find optimal batch size for your GPU\n",
        "!nvidia-smi --loop=1  # Monitor GPU memory usage"
      ],
      "metadata": {
        "id": "QXs5HGFfYII3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}